<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hadoop笔记 | Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="一、虚拟机准备1、克隆虚拟机![image-20200926164721838](file:&#x2F;&#x2F;&#x2F;Users&#x2F;lankangning&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20200926164721838.png?lastModify&#x3D;1609225369) 自定义安装，然后选择CD&#x2F;DVD为centos镜像，安装好一个后，克隆">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop笔记">
<meta property="og:url" content="http://example.com/2020/12/29/Hadoop%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="一、虚拟机准备1、克隆虚拟机![image-20200926164721838](file:&#x2F;&#x2F;&#x2F;Users&#x2F;lankangning&#x2F;Library&#x2F;Application Support&#x2F;typora-user-images&#x2F;image-20200926164721838.png?lastModify&#x3D;1609225369) 自定义安装，然后选择CD&#x2F;DVD为centos镜像，安装好一个后，克隆">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201015100849304.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201015102811297.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201015103435351.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201015103536078.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201015103819293.png?lastModify=1609225369">
<meta property="og:image" content="https://img-blog.csdnimg.cn/20190808202038101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tyaXNtaWxlX19xaA==,size_16,color_FFFFFF,t_70">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201019150624206.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/20170904183834049.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201019150650566.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/20170904184135696.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201021100427894.png?lastModify=1609225369">
<meta property="og:image" content="file:///Users/lankangning/Desktop/工作/image-20201021104659170.png?lastModify=1609225369">
<meta property="og:image" content="http://example.com/Users/lankangning/Desktop/%E5%B7%A5%E4%BD%9C/image-20201021200158943.png">
<meta property="og:image" content="http://example.com/Users/lankangning/Desktop/%E5%B7%A5%E4%BD%9C/image-20201024143514209.png">
<meta property="og:image" content="http://example.com/Users/lankangning/Desktop/%E5%B7%A5%E4%BD%9C/image-20201027104043022.png">
<meta property="og:image" content="http://example.com/Users/lankangning/Desktop/%E5%B7%A5%E4%BD%9C/image-20201028103409902.png">
<meta property="og:image" content="file:///Users/lankangning/Downloads/%E8%AE%B2%E4%B9%89/md/assets/1561705895292.png?lastModify=1603853872">
<meta property="og:image" content="http://example.com/Users/lankangning/Downloads/%E8%AE%B2%E4%B9%89/md/assets/1561705918286.png">
<meta property="og:image" content="http://example.com/Users/lankangning/Downloads/%E8%AE%B2%E4%B9%89/md/assets/1561705940248.png">
<meta property="og:image" content="http://example.com/Users/lankangning/Downloads/%E8%AE%B2%E4%B9%89/md/assets/1561705959607.png">
<meta property="article:published_time" content="2020-12-29T07:02:17.000Z">
<meta property="article:modified_time" content="2020-12-29T07:05:16.861Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="hadoop">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="file:///Users/lankangning/Desktop/工作/image-20201015100849304.png?lastModify=1609225369">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-Hadoop笔记" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2020/12/29/Hadoop%E7%AC%94%E8%AE%B0/" class="article-date">
  <time class="dt-published" datetime="2020-12-29T07:02:17.000Z" itemprop="datePublished">2020-12-29</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      Hadoop笔记
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h2 id="一、虚拟机准备"><a href="#一、虚拟机准备" class="headerlink" title="一、虚拟机准备"></a>一、虚拟机准备</h2><h3 id="1、克隆虚拟机"><a href="#1、克隆虚拟机" class="headerlink" title="1、克隆虚拟机"></a>1、克隆虚拟机</h3><p>![image-20200926164721838](file:///Users/lankangning/Library/Application Support/typora-user-images/image-20200926164721838.png?lastModify=1609225369)</p>
<p>自定义安装，然后选择CD/DVD为centos镜像，安装好一个后，克隆3个node01，node02，node03；</p>
<h3 id="2、修改静态IP"><a href="#2、修改静态IP" class="headerlink" title="2、修改静态IP"></a>2、修改静态IP</h3><ul>
<li><p>​    VMware Fusion 安装完成后，会在macOS中新建两个网卡：vmnet1以及vmnet8（在 <code>/Library/Preferences/VMware Fusion</code> 下可以看到） 1. vmnet1 是 Host-only 模式 2. vmnet8 是 NAT 模式</p>
</li>
<li><p>修改vmnet8网卡配置：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 将 DHCP 设置为 no， 即使用静态IP。 将 SUBNET 修改为自己想用的网段，比如自定义192.168.250.0网段</span><br><span class="line"># 保存退出</span><br><span class="line">sudo vi &#x2F;Library&#x2F;Preferences&#x2F;VMware\ Fusion&#x2F;networking</span><br><span class="line">answer VNET_8_DHCP no</span><br><span class="line">answer VNET_8_HOSTONLY_SUBNET 192.168.250.0</span><br></pre></td></tr></table></figure></li>
<li><p>修改vmnet8 NAT配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 设置网关为192.168.250.2，网关的IP要和上一步中的IP保持网关一致，至此VMware Fusion的配置完毕。</span><br><span class="line">sudo vi &#x2F;Library&#x2F;Preferences&#x2F;VMware\ Fusion&#x2F;vmnet8&#x2F;nat.conf</span><br><span class="line"></span><br><span class="line"># NAT gateway address</span><br><span class="line">ip &#x3D; 192.168.250.2</span><br><span class="line">netmask &#x3D; 255.255.255.0</span><br></pre></td></tr></table></figure></li>
<li><p>将node01，node02，node03切换为NAT模式</p>
</li>
<li><p>修改node01，node02，node03的网络配置</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vi ..&#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-ens33</span><br><span class="line"></span><br><span class="line">BOOTPROTO&#x3D;static &#x2F;&#x2F;dhcp改为static</span><br><span class="line">ONBOOT&#x3D;yes  &#x2F;&#x2F;no改为yes</span><br><span class="line">IPADDR&#x3D;192.168.250.110&#x2F;120&#x2F;130      &#x2F;&#x2F; 自定义的虚拟机IP， 需与VMware Fusion配置的IP在同一个网段上(node01-110，node02-120，node03-130)</span><br><span class="line">GATEWAY&#x3D;192.168.250.2       &#x2F;&#x2F; 配置的网关地址</span><br><span class="line">NETMASK&#x3D;255.255.255.0       &#x2F;&#x2F; 配置的掩码</span><br><span class="line">DNS1&#x3D;114.114.114.114        &#x2F;&#x2F; macOS本机的DNS地址</span><br><span class="line">DNS2&#x3D;8.8.8.8                &#x2F;&#x2F; 同上</span><br></pre></td></tr></table></figure>


</li>
</ul>
<ul>
<li>ping baidu.com 成功</li>
</ul>
<h3 id="3、修改主机名和hosts"><a href="#3、修改主机名和hosts" class="headerlink" title="3、修改主机名和hosts"></a>3、修改主机名和hosts</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">vi ..&#x2F;etc&#x2F;sysconfig&#x2F;network</span><br><span class="line"></span><br><span class="line">HOSTNAME&#x3D;node01&#x2F;node02&#x2F;node03</span><br><span class="line"></span><br><span class="line">vi ..&#x2F;etc&#x2F;hosts</span><br><span class="line"></span><br><span class="line">192.168.250.110 node01 node01.hadoop.com</span><br><span class="line">192.168.250.120 node02 node02.hadoop.com</span><br><span class="line">192.168.250.130 node03 node03.hadoop.com</span><br><span class="line"></span><br><span class="line">ping node01</span><br><span class="line">reboot</span><br><span class="line"># 成功</span><br></pre></td></tr></table></figure>
<h3 id="4、关闭防火墙-amp-selinux"><a href="#4、关闭防火墙-amp-selinux" class="headerlink" title="4、关闭防火墙&amp;selinux"></a>4、关闭防火墙&amp;selinux</h3><p>centos8关闭防火墙</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld.service</span><br><span class="line">systemctl disable firewalld.service</span><br></pre></td></tr></table></figure>
<p>关闭selinux</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vi ..&#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 修改selinux为disabled</span><br><span class="line">SELINUX &#x3D; disabled</span><br></pre></td></tr></table></figure>
<h3 id="5、虚拟机免密登录"><a href="#5、虚拟机免密登录" class="headerlink" title="5、虚拟机免密登录"></a>5、虚拟机免密登录</h3><ul>
<li><p>ssh免密登录原理</p>
<ol>
<li>在B节点配置A节点的公钥</li>
<li>A节点请求登录B节点</li>
<li>B节点使用A节点的公钥加密文本，发给A节点</li>
<li>A节点使用私钥解密，发回给B节点</li>
<li>B节点验证文本是否正确</li>
</ol>
</li>
<li><p>配置免密登录</p>
<ol>
<li>生成三台节点的公钥和私钥</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<ol>
<li>拷贝三台机器的公钥到同一台机器node01</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id node01</span><br></pre></td></tr></table></figure>
<ol>
<li>复制第一台机器的认证到其他两台机器</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scp &#x2F;root&#x2F;.ssh&#x2F;authorized_keys node02:&#x2F;root&#x2F;.ssh</span><br><span class="line">scp &#x2F;root&#x2F;.ssh&#x2F;authorized_keys node03:&#x2F;root&#x2F;.ssh</span><br><span class="line"></span><br><span class="line"># 在node01上</span><br><span class="line">ssh node02</span><br><span class="line">exit</span><br><span class="line"># 成功</span><br></pre></td></tr></table></figure>
<h3 id="6、时钟同步"><a href="#6、时钟同步" class="headerlink" title="6、时钟同步"></a>6、时钟同步</h3></li>
</ul>
<p>方式1：</p>
<p>所有主机和同一台主机的时间保持同步</p>
<p>方式2：</p>
<p>所有主机和网络服务器保持同步</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ntp</span><br><span class="line"></span><br><span class="line">crontab -e</span><br><span class="line"></span><br><span class="line">#输入</span><br><span class="line">*&#x2F;1****&#x2F;usr&#x2F;sbin&#x2F;ntpdate ntp4.aliyun.com;</span><br></pre></td></tr></table></figure>
<h3 id="7、软件安装"><a href="#7、软件安装" class="headerlink" title="7、软件安装"></a>7、软件安装</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"># 安装jdk</span><br><span class="line">yum install -y java</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># mysql</span><br><span class="line"># 安装mysql</span><br><span class="line">yum install -y mysql mysql-server mysql-devel</span><br><span class="line"># 安装完成后，运行以下命令来启动MySQL服务并使它在启动时自动启动：</span><br><span class="line">sudo systemctl enable --now mysqld</span><br><span class="line"># 要检查MySQL服务器是否正在运行，请输入：</span><br><span class="line">sudo systemctl status mysqld</span><br><span class="line"># mysql安全设置</span><br><span class="line">sudo mysql_secure_installation</span><br><span class="line">#输入root密码</span><br><span class="line">#please set the password for root here.</span><br><span class="line">#new password：</span><br><span class="line">#re-enter new password：</span><br><span class="line">#Remove anonymous users? (Press y|Y for Yes, any other key for No) :</span><br><span class="line">y</span><br><span class="line">#Success.</span><br><span class="line">#Disallow root login remotely? (Press y|Y for Yes, any other key for No) : </span><br><span class="line">n</span><br><span class="line">#Remove test database and access to it? (Press y|Y for Yes, any other key for No) : </span><br><span class="line">n</span><br><span class="line">#Reload privilege tables now? (Press y|Y for Yes, any other key for No) : </span><br><span class="line">y</span><br><span class="line">#all done</span><br><span class="line">#进入mysql授权</span><br><span class="line">mysql -u root -p</span><br><span class="line"># mysql&gt;</span><br><span class="line">use mysql;</span><br><span class="line">select host,user from mysql.user;</span><br><span class="line">update user set host&#x3D;&#39;%&#39; where user&#x3D;&#39;root&#39;;</span><br><span class="line">alter user &#39;root&#39;@&#39;%&#39; identified by &#39;123&#39;;</span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; WITH GRANT OPTION;</span><br><span class="line"># 不能一行直接设置</span><br><span class="line"># 参考https:&#x2F;&#x2F;www.jianshu.com&#x2F;p&#x2F;ab02b8a5d0f1</span><br><span class="line"># 刷新权限</span><br><span class="line">flush privileges;</span><br><span class="line"></span><br><span class="line">#在node01上尝试</span><br><span class="line">mysql -h node02 -u root -p</span><br><span class="line">#成功</span><br></pre></td></tr></table></figure>
<h2 id="二、zookeeper"><a href="#二、zookeeper" class="headerlink" title="二、zookeeper"></a>二、zookeeper</h2><h3 id="1、zookeeper概述"><a href="#1、zookeeper概述" class="headerlink" title="1、zookeeper概述"></a>1、zookeeper概述</h3><p>是开源分布式协调服务框架，解决分布式集群中应用系统的一致性问题和数据管理问题</p>
<h3 id="2、zookeeper特点"><a href="#2、zookeeper特点" class="headerlink" title="2、zookeeper特点"></a>2、zookeeper特点</h3><p>分布式文件系统，适合存放小文件，也可以理解为一个数据库</p>
<ol>
<li>zookeeper是一个集群</li>
<li>每一个znode是有路径，又可以携带数据，既是文件又是文件夹</li>
</ol>
<h3 id="3、zookeeper的架构"><a href="#3、zookeeper的架构" class="headerlink" title="3、zookeeper的架构"></a>3、zookeeper的架构</h3><p>zookeeper集群是基于主从架构的高可用集群</p>
<p>leader —— follower —— observer</p>
<ol>
<li>leader，zookeeper集群同一时间只有一个leader工作，处理事务</li>
<li>follower处理非事务性任务，只能处理读请求，写请求需要转发给leader，参与写请求的投票</li>
<li>跟follower一样，不参与投票</li>
</ol>
<h3 id="4、zookeeper应用场景"><a href="#4、zookeeper应用场景" class="headerlink" title="4、zookeeper应用场景"></a>4、zookeeper应用场景</h3><ol>
<li><p>数据发布和订阅</p>
<p>两种模式：</p>
<p>推：主动将订阅发送给客户端</p>
<p>拉：客户端请求获取数据</p>
</li>
<li><p>命名服务</p>
<p>客户端可以根据<strong>指定名字</strong>来获取资源的实体，zookeeper可以实现一套分布式全局唯一ID的分配机制</p>
</li>
<li><p>分布式协调/通知</p>
<p>心跳检测：检测主机是否存活</p>
<p>工作进度汇报：</p>
<p>系统调度：控制业务逻辑</p>
</li>
<li><p>分布式锁</p>
<p>分布式锁是用于控制分布式系统之间同步访问共享资源的一种方式，可以保证不同系统访问统一资源的一致性。</p>
<ul>
<li>写锁/独占锁：若事务T1对数据对象O1加上独占锁，那么在整个加锁期间只有T1能对O1进行操作。<ul>
<li>获取锁：创建时，客户端通过调用接口创建临时节点/exclusive_lock/lock，只有一个客户端能创建成功，其余客户端注册节点/exclusive_lock进行监听。</li>
<li>释放锁：当前获取锁的客户端宕机或者事务结束之后，会导致临时节点的删除，此时所有监听/exclusive_lock的客户端会收到消息，可以重新发起获取锁的请求。</li>
</ul>
</li>
<li>共享锁：若事务T1对数据对象O1加上共享锁，那么T1事务只能对O1进行读取操作，其他事务也能给O1上共享锁，直到O1上所有共享锁都被释放。获取共享锁，所有客户端都会到/shared_lock下面创建一个临时顺序节点。</li>
</ul>
</li>
<li><p>分布式队列</p>
<p>多个团队共同完成一个任务。</p>
</li>
</ol>
<h3 id="5、zookeeper的选举机制"><a href="#5、zookeeper的选举机制" class="headerlink" title="5、zookeeper的选举机制"></a>5、zookeeper的选举机制</h3><p>leader选举是保证分布式数据一致性的关键所在，两种情况需要选举leader</p>
<ol>
<li><p>服务器启动时期的leader选举</p>
<p>选取leader时至少需要两台机器，这里server1、server2、server3，步骤如下：</p>
<p>（1）每个server发出一个投票。由于是初始情况，server1和server2都会将自己作为leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID</p>
<p>myid是每台服务器的权值，越大就越占优势，zxid是事务id，越大表示事务越新，越占优势（myid，ZXID），此时server1 （1，0)，server2（2，0）然后各自将这个投票发给集群中的其他机器。</p>
<p>（2）接受来自各个服务器的投票。判断投票的有效性，如是否本轮投票、是否来自looking状态的服务器。</p>
<p>（3）处理投票。针对每一个投票，服务器需要将别人的投票和自己的进行PK，规则如下：</p>
<ul>
<li>优先检查zxid，zxid较大的座位leader</li>
<li>zxid相同，比较myid，myid较大的作为leader</li>
</ul>
<p>没有PK胜利的要更新自己的投票，进行下一轮投票，PK胜利的无需投票</p>
<p>选举有过半机制，一般要求主机数量为奇数。</p>
</li>
</ol>
<h3 id="6、zookeeper的安装"><a href="#6、zookeeper的安装" class="headerlink" title="6、zookeeper的安装"></a>6、zookeeper的安装</h3><p>集群规划</p>
<table>
<thead>
<tr>
<th>IP</th>
<th>hostname</th>
<th>myid</th>
</tr>
</thead>
<tbody><tr>
<td>192.168.250.110</td>
<td>node01</td>
<td>1</td>
</tr>
<tr>
<td>192.168.250.120</td>
<td>node02</td>
<td>2</td>
</tr>
<tr>
<td>192.168.250.130</td>
<td>node03</td>
<td>3(leader)</td>
</tr>
</tbody></table>
<ol>
<li><p>下载zookeeper，解压 /usr/lib/apache-zookeeper-3.6.2-bin</p>
<p>cd /usr/lib/apache-zookeeper-3.6.2-bin</p>
</li>
<li><p>修改配置文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">cd conf</span><br><span class="line">cp zoo_sample.cfg zoo.cfg</span><br><span class="line">vi zoo.cfg</span><br><span class="line"></span><br><span class="line">数据目录</span><br><span class="line">dataDir&#x3D;&#x2F;tmp&#x2F;zookeeper&#x2F;zkdatas</span><br><span class="line"># The number of snapshots to retain in dataDir</span><br><span class="line"># 保留多少个快照</span><br><span class="line">autopurge.snapRetainCount&#x3D;3</span><br><span class="line"># Purge task interval in hours</span><br><span class="line"># Set to &quot;0&quot; to disable auto purge feature</span><br><span class="line"># 日志多少小时清理一次</span><br><span class="line">autopurge.purgeInterval&#x3D;1</span><br><span class="line"></span><br><span class="line"># 集群中的服务器地址</span><br><span class="line">server.1 &#x3D; node01:2888:3888</span><br><span class="line">server.2 &#x3D; node02:2888:3888</span><br><span class="line">server.3 &#x3D; node03:2888:3888</span><br></pre></td></tr></table></figure></li>
<li><p>添加myid配置</p>
<p>在node01的dataDir里创建myid文件，值为1</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;tmp&#x2F;zookeeper&#x2F;zkdatas</span><br><span class="line">vi myid</span><br><span class="line">echo 1 &gt; &#x2F;tmp&#x2F;zookeeper&#x2F;zkdatas&#x2F;myid</span><br><span class="line"></span><br><span class="line"># node01为1，node02为2，node03为3</span><br></pre></td></tr></table></figure></li>
<li><p>启动zookeeper</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;lib&#x2F;apache-zookeeper-3.6.2-bin&#x2F;bin</span><br><span class="line">.&#x2F;zkServer.sh start</span><br><span class="line"># 查看是否启动</span><br><span class="line">jps</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="7、zookeeper的数据模型"><a href="#7、zookeeper的数据模型" class="headerlink" title="7、zookeeper的数据模型"></a>7、zookeeper的数据模型</h3><p>zookeeper的每个节点称为znode，结构类似linux文件系统，树形结构</p>
<p>不同：</p>
<ol>
<li><p>znode具有文件和目录两种特点。每个节点可以存放一些数据，如元信息、acl、时间戳等。又可以作为目录，拥有子节点。可以对znode进行增删改查</p>
</li>
<li><p>znode存储数据大小有限制。存储的数据一般是存储信息，状态信息，调度数据等，数据量都很小，至多1M，一般以kb为大小单位</p>
</li>
<li><p>znode通过路径 引用。路径必须为绝对路径，必须是唯一的。某些节点是有限制的，如/zookeeper 用来保存管理信息，如关键配额信息。是不能删除的。</p>
</li>
<li><p>znode由3部分组成</p>
<p>stat：状态信息，描述该znode的版本、权限等信息</p>
<p>data：与该znode关联的数据</p>
<p>children：该znode下的子节点</p>
</li>
</ol>
<h3 id="8、znode节点类型"><a href="#8、znode节点类型" class="headerlink" title="8、znode节点类型"></a>8、znode节点类型</h3><ol>
<li><p>临时节点</p>
<p>该节点生命周期依赖于创建它们的回话，结束回话或者手动删除会使临时节点删除。<strong>临时节点不允许拥有子节点。</strong></p>
</li>
<li><p>永久节点</p>
<p>不依赖于会话，只有在客户端执行删除，才会被删除</p>
</li>
<li><p>序列化特点</p>
<p>记录每个子节点创建的先后顺序，%10d，十位数字，00000000001</p>
<p>persistent：永久节点</p>
<p>ephemeral：临时节点</p>
<p>persistent_sequential：永久节点</p>
<p>ephemeral_sequential：临时节点</p>
</li>
</ol>
<h3 id="9、zookeeper客户端shell操作"><a href="#9、zookeeper客户端shell操作" class="headerlink" title="9、zookeeper客户端shell操作"></a>9、zookeeper客户端shell操作</h3><ol>
<li><p>登录client客户端</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;lib&#x2F;apache-zookeeper-3.6.2-bin\</span><br><span class="line"># zoo.conf配置文件里 clientport &#x3D; 2181</span><br><span class="line">bin&#x2F;zkCli.sh -server node01:2181</span><br></pre></td></tr></table></figure></li>
<li><p>客户端操作命令</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 创建znode -s是指定节点序列化 -e是指定临时节点</span><br><span class="line"># 什么都不加 创建永久节点</span><br><span class="line">create [-s] [-e] path data acl</span><br><span class="line"></span><br><span class="line">create &#x2F;hello world</span><br><span class="line"></span><br><span class="line">create -e &#x2F;tmp world</span><br><span class="line"></span><br><span class="line"># 创建子节点 只能给永久节点创建</span><br><span class="line">create &#x2F;hello&#x2F;app1 world</span><br><span class="line"></span><br><span class="line"># 修改节点数据</span><br><span class="line">set path data</span><br><span class="line">set &#x2F;hello world1</span><br><span class="line"># 获取节点数据</span><br><span class="line">get path</span><br><span class="line"># 删除节点，要保证无子节点</span><br><span class="line">delete &#x2F;hello</span><br><span class="line"># 删除节点，递归删除子节点</span><br><span class="line">rmr &#x2F;hello</span><br><span class="line"># 列出历史记录</span><br><span class="line">history</span><br><span class="line"></span><br><span class="line"># 列出path下所有znode</span><br><span class="line">ls path</span><br><span class="line"></span><br><span class="line"># 退出</span><br><span class="line">quit</span><br></pre></td></tr></table></figure>
<h3 id="10、zookeeper节点属性"><a href="#10、zookeeper节点属性" class="headerlink" title="10、zookeeper节点属性"></a>10、zookeeper节点属性</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">get -s &#x2F;hello</span><br><span class="line"></span><br><span class="line"># output</span><br><span class="line">[zk: node01:2181(CONNECTED) 15] get -s &#x2F;hello</span><br><span class="line">word</span><br><span class="line"># znode创建的事务id</span><br><span class="line">cZxid &#x3D; 0x100000006</span><br><span class="line"># 创建znode的时间戳</span><br><span class="line">ctime &#x3D; Wed Sep 30 08:33:03 CST 2020</span><br><span class="line"># znode被修改的事务id，值越大，表示更新的数据越新</span><br><span class="line">mZxid &#x3D; 0x100000008</span><br><span class="line"># 修改znode的时间戳</span><br><span class="line">mtime &#x3D; Wed Sep 30 08:33:14 CST 2020</span><br><span class="line"># 表示该节点的子节点列表最后一次修改的事务ID，添加子节点或删除子节点就会影响子节点列表，但是修改子节点的数据内容则不影响该ID（注意，只有子节点列表变更了才会变更pzxid，子节点内容变更不会影响pzxid）</span><br><span class="line">pZxid &#x3D; 0x100000006</span><br><span class="line"># 子节点版本号，当znode子节点有变化时，值+1</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line"># 数据版本号，每进行一次set操作，就+1</span><br><span class="line">dataVersion &#x3D; 1</span><br><span class="line"># acl权限版本号，修改权限每次增加1</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line"># 如果该节点为临时节点，表示与该节点绑定的会话id，如果不是，则为0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 4</span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure>
<h3 id="11、watch机制"><a href="#11、watch机制" class="headerlink" title="11、watch机制"></a>11、watch机制</h3><p>对某个znode设置watcher，当znode发生变化时，watchmanager会调用对应的watcher，当znode发生删除创建修改，子节点修改时候，watcher会得到通知。</p>
<ol>
<li>发布和订阅</li>
<li>监控集群中主机的存活状态</li>
</ol>
<p>watcher特点：</p>
<ol>
<li>一次性只能触发一个watcher，一个watcher只能触发一次，如果继续监听需要再次添加watcher</li>
<li>watcher得到的时间是被封装过的，包括三个内容keeperstate，eventtype，path</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">get -w &#x2F;hello</span><br><span class="line">set &#x2F;hello world1</span><br><span class="line"></span><br><span class="line"># output</span><br><span class="line"># keeperstate:SyncConnected</span><br><span class="line"># eventType:NodeDataChanged</span><br><span class="line"># path:&#x2F;hello</span><br><span class="line"></span><br><span class="line">[zk: node01:2181(CONNECTED) 41] </span><br><span class="line">WATCHER::</span><br><span class="line"></span><br><span class="line">WatchedEvent state:SyncConnected type:NodeDataChanged path:&#x2F;hello</span><br></pre></td></tr></table></figure>
<h3 id="12、zookeeper-JavaAPI操作"><a href="#12、zookeeper-JavaAPI操作" class="headerlink" title="12、zookeeper JavaAPI操作"></a>12、zookeeper JavaAPI操作</h3><p>curator包依赖</p>
<p>往maven里添加curator-framework、curator-recipes、google-collections、slf4j-simple</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependencies&gt;</span><br><span class="line">        &lt;!-- https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;org.apache.curator&#x2F;curator-recipes --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.curator&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;curator-recipes&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;5.1.0&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;!-- https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;org.apache.curator&#x2F;curator-framework --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.apache.curator&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;curator-framework&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;5.1.0&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;!-- https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;com.google.collections&#x2F;google-collections --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;com.google.collections&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;google-collections&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;1.0&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;4.13&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">        &lt;!-- https:&#x2F;&#x2F;mvnrepository.com&#x2F;artifact&#x2F;org.slf4j&#x2F;slf4j-simple --&gt;</span><br><span class="line">        &lt;dependency&gt;</span><br><span class="line">            &lt;groupId&gt;org.slf4j&lt;&#x2F;groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;slf4j-simple&lt;&#x2F;artifactId&gt;</span><br><span class="line">            &lt;version&gt;2.0.0-alpha1&lt;&#x2F;version&gt;</span><br><span class="line">        &lt;&#x2F;dependency&gt;</span><br><span class="line">    &lt;&#x2F;dependencies&gt;</span><br></pre></td></tr></table></figure>
<p>创建java project</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zookeeper&#x2F;src&#x2F;main&#x2F;java&#x2F;com.lan.zookeeper_api&#x2F;ZookeeperAPITest.class</span><br></pre></td></tr></table></figure>


<ol>
<li><p>节点的操作</p>
<ul>
<li>创建永久节点</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">package com.lan.zookeeper_api;</span><br><span class="line"></span><br><span class="line">import org.apache.curator.RetryPolicy;</span><br><span class="line">import org.apache.curator.RetrySleeper;</span><br><span class="line">import org.apache.curator.framework.CuratorFramework;</span><br><span class="line">import org.apache.curator.framework.CuratorFrameworkFactory;</span><br><span class="line">import org.apache.curator.retry.ExponentialBackoffRetry;</span><br><span class="line">import org.apache.zookeeper.CreateMode;</span><br><span class="line">import org.apache.zookeeper.Op;</span><br><span class="line">import org.junit.Test;</span><br><span class="line"></span><br><span class="line">import java.util.Random;</span><br><span class="line"></span><br><span class="line">public class ZookeeperAPITest &#123;</span><br><span class="line">    @Test</span><br><span class="line">    public void createZnode()&#123;</span><br><span class="line">        &#x2F;&#x2F;1：制定重试策略</span><br><span class="line">        RetryPolicy retryPolicy &#x3D; new ExponentialBackoffRetry(1000,3);</span><br><span class="line">&#x2F;&#x2F; @param baseSleepTimeMs initial amount of time to wait between retries</span><br><span class="line">&#x2F;&#x2F; @param maxRetries max number of times to retry</span><br><span class="line">&#x2F;&#x2F; @param maxSleepMs max time in ms to sleep on each retry 每次重试的睡眠时间</span><br><span class="line">        &#x2F;&#x2F;2：获取一个客户端client对象</span><br><span class="line">        String connectString &#x3D; &quot;192.168.250.110:2181,192.168.250.120:2181,192.168.250.130:2181&quot;;</span><br><span class="line">        int sessionTimeoutMs &#x3D; 8000;</span><br><span class="line">        int connectionTimeoutMs &#x3D; 8000;</span><br><span class="line">        CuratorFramework client &#x3D; CuratorFrameworkFactory.newClient(connectString,</span><br><span class="line">                sessionTimeoutMs,connectionTimeoutMs,retryPolicy);</span><br><span class="line">&#x2F;&#x2F;         * @param connectString  list of servers to connect to 字符串，zookeeper服务器列表</span><br><span class="line">&#x2F;&#x2F;         * @param sessionTimeoutMs session timeout    会话超时时间</span><br><span class="line">&#x2F;&#x2F;         * @param connectionTimeoutMs connection timeout    链接超时时间</span><br><span class="line">&#x2F;&#x2F;         * @param retryPolicy  retry policy to use   使用的重试策略</span><br><span class="line">        &#x2F;&#x2F;3：开启客户端</span><br><span class="line">        client.start();</span><br><span class="line">        &#x2F;&#x2F;4：创建znode</span><br><span class="line">&#x2F;&#x2F;        PERSISTENT(0, false, false, false, false), &#x2F;&#x2F;永久节点</span><br><span class="line">&#x2F;&#x2F;        PERSISTENT_SEQUENTIAL(2, false, true, false, false),&#x2F;&#x2F;永久序列节点</span><br><span class="line">&#x2F;&#x2F;        EPHEMERAL(1, true, false, false, false),&#x2F;&#x2F;临时节点</span><br><span class="line">&#x2F;&#x2F;        EPHEMERAL_SEQUENTIAL(3, true, true, false, false),&#x2F;&#x2F;临时序列节点</span><br><span class="line">&#x2F;&#x2F;        CONTAINER(4, false, false, true, false)</span><br><span class="line">&#x2F;&#x2F;        PERSISTENT_WITH_TTL(5, false, false, false, true)</span><br><span class="line">&#x2F;&#x2F;        PERSISTENT_SEQUENTIAL_WITH_TTL(6, false, true, false, true)</span><br><span class="line">        &#x2F;&#x2F; path：string ,data: byte []</span><br><span class="line">        try &#123;</span><br><span class="line">client.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT).forPath(&quot;&#x2F;hello2&quot;,&quot;world2&quot;.getBytes());</span><br><span class="line">        &#125; catch (Exception e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">        &#x2F;&#x2F;5：关闭客户端</span><br><span class="line">        client.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上虚拟机查看zookeeper节点</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[zk: node01:2181(CONNECTED) 0] ls &#x2F;</span><br><span class="line">[hello, hello2, zookeeper]</span><br><span class="line">[zk: node01:2181(CONNECTED) 2] get -s &#x2F;hello2</span><br><span class="line">world2</span><br><span class="line">cZxid &#x3D; 0x10000000f</span><br><span class="line">ctime &#x3D; Wed Sep 30 10:48:36 CST 2020</span><br><span class="line">mZxid &#x3D; 0x10000000f</span><br><span class="line">mtime &#x3D; Wed Sep 30 10:48:36 CST 2020</span><br><span class="line">pZxid &#x3D; 0x10000000f</span><br><span class="line">cversion &#x3D; 0</span><br><span class="line">dataVersion &#x3D; 0</span><br><span class="line">aclVersion &#x3D; 0</span><br><span class="line">ephemeralOwner &#x3D; 0x0</span><br><span class="line">dataLength &#x3D; 6</span><br><span class="line">numChildren &#x3D; 0</span><br></pre></td></tr></table></figure>
<p>创建成功</p>
<ul>
<li>创建临时节点</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">public void createTmpZnode() throws InterruptedException &#123;</span><br><span class="line">  &#x2F;&#x2F;1：制定重试策略</span><br><span class="line">    RetryPolicy retryPolicy &#x3D; new ExponentialBackoffRetry(1000,3);</span><br><span class="line">    &#x2F;&#x2F;2：获取一个客户端client对象</span><br><span class="line">    String connectString &#x3D; &quot;192.168.250.110:2181,192.168.250.120:2181,192.168.250.130:2181&quot;;</span><br><span class="line">    int sessionTimeoutMs &#x3D; 8000;</span><br><span class="line">    int connectionTimeoutMs &#x3D; 8000;</span><br><span class="line">    CuratorFramework client &#x3D; CuratorFrameworkFactory.newClient(connectString,</span><br><span class="line">                                                                sessionTimeoutMs,connectionTimeoutMs,retryPolicy);</span><br><span class="line">    &#x2F;&#x2F;3：开启客户端</span><br><span class="line">    client.start();</span><br><span class="line">    &#x2F;&#x2F;4：创建znode,这里CreateMode.PERSISTENT要选EPHEMERAL 临时节点</span><br><span class="line">    try &#123;</span><br><span class="line">      client.create().creatingParentsIfNeeded().withMode(CreateMode.EPHEMERAL).forPath(&quot;&#x2F;hello4&quot;,&quot;world4&quot;.getBytes());</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F; 进程停止5秒钟</span><br><span class="line">    Thread.sleep(1000);</span><br><span class="line">    &#x2F;&#x2F;5：关闭客户端，临时节点删除</span><br><span class="line">    client.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>修改节点数据（set)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">    设置节点数据 set path data</span><br><span class="line">*&#x2F;</span><br><span class="line">@Test</span><br><span class="line">public void setZnodeData() throws Exception &#123;</span><br><span class="line">    &#x2F;&#x2F;1：设置重试策略</span><br><span class="line">    RetryPolicy retryPolicy &#x3D; new ExponentialBackoffRetry(1000, 1);</span><br><span class="line">    &#x2F;&#x2F;2：获取客户端对象</span><br><span class="line">    String connectString &#x3D; &quot;192.168.250.110:2181,192.168.250.120:2181,192.168.250.130:2181&quot;;</span><br><span class="line">    int sessionTimeoutMs &#x3D; 8000;</span><br><span class="line">    int connectionTimeoutMs &#x3D; 8000;</span><br><span class="line">    CuratorFramework client &#x3D; CuratorFrameworkFactory.newClient(connectString, sessionTimeoutMs, connectionTimeoutMs, retryPolicy);</span><br><span class="line">    &#x2F;&#x2F;3：启动client</span><br><span class="line">    client.start();</span><br><span class="line">    &#x2F;&#x2F;4：执行set操作</span><br><span class="line">    try &#123;</span><br><span class="line">      client.setData().forPath(&quot;&#x2F;hello2&quot;, &quot;abc&quot;.getBytes());</span><br><span class="line">    &#125; catch (Exception e) &#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;5：关闭client</span><br><span class="line">    client.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>获取节点数据（get)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">@Test</span><br><span class="line">        public void getZnodeData() throws Exception &#123;</span><br><span class="line">            &#x2F;&#x2F;1：设置重试策略</span><br><span class="line">            RetryPolicy retryPolicy &#x3D; new ExponentialBackoffRetry(1000, 1);</span><br><span class="line">            &#x2F;&#x2F;2：获取客户端对象</span><br><span class="line">            String connectString &#x3D; &quot;192.168.250.110:2181,192.168.250.120:2181,192.168.250.130:2181&quot;;</span><br><span class="line">            int sessionTimeoutMs &#x3D; 8000;</span><br><span class="line">            int connectionTimeoutMs &#x3D; 8000;</span><br><span class="line">            CuratorFramework client &#x3D; CuratorFrameworkFactory.newClient(connectString,sessionTimeoutMs,connectionTimeoutMs,retryPolicy);</span><br><span class="line">            &#x2F;&#x2F;3：启动client</span><br><span class="line">            client.start();</span><br><span class="line">            &#x2F;&#x2F;4：执行get操作</span><br><span class="line">            try &#123;</span><br><span class="line">              	&#x2F;&#x2F; 获取数据，byte数组格式</span><br><span class="line">                byte[] forPath &#x3D; client.getData().forPath(&quot;&#x2F;hello2&quot;);</span><br><span class="line">                System.out.println(new String(forPath));</span><br><span class="line">            &#125; catch (Exception e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">            &#x2F;&#x2F;5：关闭client</span><br><span class="line">            client.close();</span><br><span class="line">        &#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>节点watch机制</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">    节点watch机制</span><br><span class="line">*&#x2F;</span><br><span class="line">@Test</span><br><span class="line">public void watchZnode() throws Exception &#123;</span><br><span class="line">    &#x2F;&#x2F;1：设置重试策略</span><br><span class="line">    RetryPolicy retryPolicy &#x3D; new ExponentialBackoffRetry(1000, 1);</span><br><span class="line">    &#x2F;&#x2F;2：获取客户端对象</span><br><span class="line">    String connectString &#x3D; &quot;192.168.250.110:2181,192.168.250.120:2181,192.168.250.130:2181&quot;;</span><br><span class="line">    int sessionTimeoutMs &#x3D; 8000;</span><br><span class="line">    int connectionTimeoutMs &#x3D; 8000;</span><br><span class="line">    CuratorFramework client &#x3D; CuratorFrameworkFactory.newClient(connectString, sessionTimeoutMs, connectionTimeoutMs, retryPolicy);</span><br><span class="line">    &#x2F;&#x2F;3：启动client</span><br><span class="line">    client.start();</span><br><span class="line">    &#x2F;&#x2F;4：创建一个Treecache对象，指定要监控的节点路径</span><br><span class="line">    &#x2F;&#x2F; treecache弃用了，改为CuratorCache</span><br><span class="line">  	&#x2F;&#x2F; 这一段自己摸索的 还有一个CuratorCachebuilder是什么不知道</span><br><span class="line">  </span><br><span class="line">    CuratorCache cache &#x3D; CuratorCache.build(client,&quot;&#x2F;hello2&quot;);</span><br><span class="line">    cache.listenable().addListener(new CuratorCacheListener() &#123;</span><br><span class="line">      @Override</span><br><span class="line">      public void event(Type type, ChildData oldData, ChildData data) &#123;</span><br><span class="line">        switch (type)&#123;</span><br><span class="line">          case NODE_CHANGED:</span><br><span class="line">            System.out.println(&quot;有节点更变&quot;);</span><br><span class="line">            break;</span><br><span class="line">          case NODE_CREATED:</span><br><span class="line">            System.out.println(&quot;有节点创建&quot;);</span><br><span class="line">            break;</span><br><span class="line">          case NODE_DELETED:</span><br><span class="line">            System.out.println(&quot;有节点被删除&quot;);</span><br><span class="line">            break;</span><br><span class="line">          default:</span><br><span class="line">            break;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;);</span><br><span class="line">    &#x2F;&#x2F;5：自定义一个监听器</span><br><span class="line">    cache.start();</span><br><span class="line">    Thread.sleep(100000000);</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;6：关闭client</span><br><span class="line">    &#x2F;&#x2F;        client.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h2 id="三、Hadoop"><a href="#三、Hadoop" class="headerlink" title="三、Hadoop"></a>三、Hadoop</h2><p>hdfs，MapReduce，yarn</p>
<h3 id="1、hadoop1-x架构"><a href="#1、hadoop1-x架构" class="headerlink" title="1、hadoop1.x架构"></a>1、hadoop1.x架构</h3><p><img src="file:///Users/lankangning/Desktop/工作/image-20201015100849304.png?lastModify=1609225369" alt="image-20201015100849304"></p>
<ul>
<li><p>HDFS：</p>
<p>namenode：集群的主节点，管理元数据（文件大小，位置，权限）主要用于管理集群中的各种数据。</p>
<p>secondaryNameNode：主要用于hadoop当中元数据的辅助管理，减轻namenode的压力</p>
<p>DataNode：集群的从节点，主要存储数据</p>
</li>
<li><p>MapReduce：</p>
<p>jobTracker：接受用户的计算请求任务，并分配任务给从节点</p>
<p>tasktracker：主要负责运算的节点</p>
</li>
</ul>
<h3 id="2、hadoop2-x架构"><a href="#2、hadoop2-x架构" class="headerlink" title="2、hadoop2.x架构"></a>2、hadoop2.x架构</h3><ul>
<li><p>NameNode和ResourceManager单节点架构</p>
<p><img src="file:///Users/lankangning/Desktop/工作/image-20201015102811297.png?lastModify=1609225369" alt="image-20201015102811297"></p>
<p>resourceManager：接受用户的计算请求任务，并负责集群的资源分配</p>
<p>nodemanager：负责执行主节点APPmaster分配的任务</p>
</li>
<li><p>NameNode单节点与resourceManager高可用架构模型</p>
<p><img src="file:///Users/lankangning/Desktop/工作/image-20201015103435351.png?lastModify=1609225369" alt="image-20201015103435351"></p>
<p>zookeeper，wathc机制实现主节点和备份节点的监听，resourceManager主节点一旦挂掉，就使用备份节点</p>
</li>
<li><p>NameNode高可用与ResourceManager单节点架构模型</p>
</li>
</ul>
<p><img src="file:///Users/lankangning/Desktop/工作/image-20201015103536078.png?lastModify=1609225369" alt="image-20201015103536078"></p>
<p>​    元数据信息管理保持主节点和备份节点数据一致</p>
<ul>
<li><p>NameNode和ResourceManager高可用架构模型</p>
<p><img src="file:///Users/lankangning/Desktop/工作/image-20201015103819293.png?lastModify=1609225369" alt="image-20201015103819293"></p>
</li>
</ul>
<h3 id="3、hadoop编译"><a href="#3、hadoop编译" class="headerlink" title="3、hadoop编译"></a>3、hadoop编译</h3><p>环境：centos8.2.2004，VMware fusion 12，hadoop2.10.1</p>
<p>所有的包都放在/usr/lib下</p>
<ol>
<li><p>准备工作</p>
<ul>
<li>安装java jdk-1.8.0</li>
</ul>
<p><img src="https://img-blog.csdnimg.cn/20190808202038101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2tyaXNtaWxlX19xaA==,size_16,color_FFFFFF,t_70" alt="img"></p>
<ul>
<li><p>安装maven</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf apache-maven-3.6.3-bin-tar.gz</span><br><span class="line"># 配置环境变量</span><br><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line">export MAVEN_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;apache-maven-3.6.3</span><br><span class="line">export PATH:$MAVEN_HOME&#x2F;bin:$PATH</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br><span class="line"></span><br><span class="line"># 测试</span><br><span class="line">mvn -v</span><br></pre></td></tr></table></figure>
<p>配置maven</p>
<p>修改maven文件夹下conf/settings.xml</p>
<p>设置本地仓库/usr/lib/maven-repo,阿里源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--</span><br><span class="line">Licensed to the Apache Software Foundation (ASF) under one</span><br><span class="line">or more contributor license agreements.  See the NOTICE file</span><br><span class="line">distributed with this work for additional information</span><br><span class="line">regarding copyright ownership.  The ASF licenses this file</span><br><span class="line">to you under the Apache License, Version 2.0 (the</span><br><span class="line">&quot;License&quot;); you may not use this file except in compliance</span><br><span class="line">with the License.  You may obtain a copy of the License at</span><br><span class="line"></span><br><span class="line">    http:&#x2F;&#x2F;www.apache.org&#x2F;licenses&#x2F;LICENSE-2.0</span><br><span class="line"></span><br><span class="line">Unless required by applicable law or agreed to in writing,</span><br><span class="line">software distributed under the License is distributed on an</span><br><span class="line">&quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY</span><br><span class="line">KIND, either express or implied.  See the License for the</span><br><span class="line">specific language governing permissions and limitations</span><br><span class="line">under the License.</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--</span><br><span class="line"> | This is the configuration file for Maven. It can be specified at two levels:</span><br><span class="line"> |</span><br><span class="line"> |  1. User Level. This settings.xml file provides configuration for a single user,</span><br><span class="line"> |                 and is normally provided in $&#123;user.home&#125;&#x2F;.m2&#x2F;settings.xml.</span><br><span class="line"> |</span><br><span class="line"> |                 NOTE: This location can be overridden with the CLI option:</span><br><span class="line"> |</span><br><span class="line"> |                 -s &#x2F;path&#x2F;to&#x2F;user&#x2F;settings.xml</span><br><span class="line"> |</span><br><span class="line"> |  2. Global Level. This settings.xml file provides configuration for all Maven</span><br><span class="line"> |                 users on a machine (assuming they&#39;re all using the same Maven</span><br><span class="line"> |                 installation). It&#39;s normally provided in</span><br><span class="line"> |                 $&#123;maven.conf&#125;&#x2F;settings.xml.</span><br><span class="line"> |</span><br><span class="line"> |                 NOTE: This location can be overridden with the CLI option:</span><br><span class="line"> |</span><br><span class="line"> |                 -gs &#x2F;path&#x2F;to&#x2F;global&#x2F;settings.xml</span><br><span class="line"> |</span><br><span class="line"> | The sections in this sample file are intended to give you a running start at</span><br><span class="line"> | getting the most out of your Maven installation. Where appropriate, the default</span><br><span class="line"> | values (values used when the setting is not specified) are provided.</span><br><span class="line"> |</span><br><span class="line"> |--&gt;</span><br><span class="line">&lt;settings xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;SETTINGS&#x2F;1.0.0&quot;</span><br><span class="line">          xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class="line">          xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;SETTINGS&#x2F;1.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;xsd&#x2F;settings-1.0.0.xsd&quot;&gt;</span><br><span class="line">  &lt;!-- localRepository</span><br><span class="line">   | The path to the local repository maven will use to store artifacts.</span><br><span class="line">   |</span><br><span class="line">   | Default: $&#123;user.home&#125;&#x2F;.m2&#x2F;repository</span><br><span class="line">  &lt;localRepository&gt;&#x2F;path&#x2F;to&#x2F;local&#x2F;repo&lt;&#x2F;localRepository&gt;</span><br><span class="line">  --&gt;</span><br><span class="line">  &lt;localRepository&gt;&#x2F;usr&#x2F;lib&#x2F;maven-repo&lt;&#x2F;localRepository&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- interactiveMode</span><br><span class="line">   | This will determine whether maven prompts you when it needs input. If set to false,</span><br><span class="line">   | maven will use a sensible default value, perhaps based on some other setting, for</span><br><span class="line">   | the parameter in question.</span><br><span class="line">   |</span><br><span class="line">   | Default: true</span><br><span class="line">  &lt;interactiveMode&gt;true&lt;&#x2F;interactiveMode&gt;</span><br><span class="line">  --&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- offline</span><br><span class="line">   | Determines whether maven should attempt to connect to the network when executing a build.</span><br><span class="line">   | This will have an effect on artifact downloads, artifact deployment, and others.</span><br><span class="line">   |</span><br><span class="line">   | Default: false</span><br><span class="line">  &lt;offline&gt;false&lt;&#x2F;offline&gt;</span><br><span class="line">  --&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- pluginGroups</span><br><span class="line">   | This is a list of additional group identifiers that will be searched when resolving plugins by their prefix, i.e.</span><br><span class="line">   | when invoking a command line like &quot;mvn prefix:goal&quot;. Maven will automatically add the group identifiers</span><br><span class="line">   | &quot;org.apache.maven.plugins&quot; and &quot;org.codehaus.mojo&quot; if these are not already contained in the list.</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;pluginGroups&gt;</span><br><span class="line">    &lt;!-- pluginGroup</span><br><span class="line">     | Specifies a further group identifier to use for plugin lookup.</span><br><span class="line">    &lt;pluginGroup&gt;com.your.plugins&lt;&#x2F;pluginGroup&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">  &lt;&#x2F;pluginGroups&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- proxies</span><br><span class="line">   | This is a list of proxies which can be used on this machine to connect to the network.</span><br><span class="line">   | Unless otherwise specified (by system property or command-line switch), the first proxy</span><br><span class="line">   | specification in this list marked as active will be used.</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;proxies&gt;</span><br><span class="line">    &lt;!-- proxy</span><br><span class="line">     | Specification for one proxy, to be used in connecting to the network.</span><br><span class="line">     |</span><br><span class="line">    &lt;proxy&gt;</span><br><span class="line">      &lt;id&gt;optional&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;active&gt;true&lt;&#x2F;active&gt;</span><br><span class="line">      &lt;protocol&gt;http&lt;&#x2F;protocol&gt;</span><br><span class="line">      &lt;username&gt;proxyuser&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;proxypass&lt;&#x2F;password&gt;</span><br><span class="line">      &lt;host&gt;proxy.host.net&lt;&#x2F;host&gt;</span><br><span class="line">      &lt;port&gt;80&lt;&#x2F;port&gt;</span><br><span class="line">      &lt;nonProxyHosts&gt;local.net|some.host.com&lt;&#x2F;nonProxyHosts&gt;</span><br><span class="line">    &lt;&#x2F;proxy&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">  &lt;&#x2F;proxies&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- servers</span><br><span class="line">   | This is a list of authentication profiles, keyed by the server-id used within the system.</span><br><span class="line">   | Authentication profiles can be used whenever maven must make a connection to a remote server.</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;servers&gt;</span><br><span class="line">    &lt;!-- server</span><br><span class="line">     | Specifies the authentication information to use when connecting to a particular server, identified by</span><br><span class="line">     | a unique name within the system (referred to by the &#39;id&#39; attribute below).</span><br><span class="line">     |</span><br><span class="line">     | NOTE: You should either specify username&#x2F;password OR privateKey&#x2F;passphrase, since these pairings are</span><br><span class="line">     |       used together.</span><br><span class="line">     |</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;deploymentRepo&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;username&gt;repouser&lt;&#x2F;username&gt;</span><br><span class="line">      &lt;password&gt;repopwd&lt;&#x2F;password&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">    --&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!-- Another sample, using keys to authenticate.</span><br><span class="line">    &lt;server&gt;</span><br><span class="line">      &lt;id&gt;siteServer&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;privateKey&gt;&#x2F;path&#x2F;to&#x2F;private&#x2F;key&lt;&#x2F;privateKey&gt;</span><br><span class="line">      &lt;passphrase&gt;optional; leave empty if not used.&lt;&#x2F;passphrase&gt;</span><br><span class="line">    &lt;&#x2F;server&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">  &lt;&#x2F;servers&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- mirrors</span><br><span class="line">   | This is a list of mirrors to be used in downloading artifacts from remote repositories.</span><br><span class="line">   |</span><br><span class="line">   | It works like this: a POM may declare a repository to use in resolving certain artifacts.</span><br><span class="line">   | However, this repository may have problems with heavy traffic at times, so people have mirrored</span><br><span class="line">   | it to several places.</span><br><span class="line">   |</span><br><span class="line">   | That repository definition will have a unique id, so we can create a mirror reference for that</span><br><span class="line">   | repository, to be used as an alternate download site. The mirror site will be the preferred</span><br><span class="line">   | server for that repository.</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;mirrors&gt;</span><br><span class="line">    &lt;!-- mirror</span><br><span class="line">     | Specifies a repository mirror site to use instead of a given repository. The repository that</span><br><span class="line">     | this mirror serves has an ID that matches the mirrorOf element of this mirror. IDs are used</span><br><span class="line">     | for inheritance and direct lookup purposes, and must be unique across the set of mirrors.</span><br><span class="line">     |--&gt;</span><br><span class="line">    &lt;mirror&gt;</span><br><span class="line">      &lt;id&gt;aliyunmaven&lt;&#x2F;id&gt;</span><br><span class="line">      &lt;mirrorOf&gt;*&lt;&#x2F;mirrorOf&gt;</span><br><span class="line">      &lt;name&gt;阿里云公共仓库&lt;&#x2F;name&gt;</span><br><span class="line">      &lt;url&gt;https:&#x2F;&#x2F;maven.aliyun.com&#x2F;repository&#x2F;public&lt;&#x2F;url&gt;</span><br><span class="line">    &lt;&#x2F;mirror&gt;</span><br><span class="line">  &lt;&#x2F;mirrors&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- profiles</span><br><span class="line">   | This is a list of profiles which can be activated in a variety of ways, and which can modify</span><br><span class="line">   | the build process. Profiles provided in the settings.xml are intended to provide local machine-</span><br><span class="line">   | specific paths and repository locations which allow the build to work in the local environment.</span><br><span class="line">   |</span><br><span class="line">   | For example, if you have an integration testing plugin - like cactus - that needs to know where</span><br><span class="line">   | your Tomcat instance is installed, you can provide a variable here such that the variable is</span><br><span class="line">   | dereferenced during the build process to configure the cactus plugin.</span><br><span class="line">   |</span><br><span class="line">   | As noted above, profiles can be activated in a variety of ways. One way - the activeProfiles</span><br><span class="line">   | section of this document (settings.xml) - will be discussed later. Another way essentially</span><br><span class="line">   | relies on the detection of a system property, either matching a particular value for the property,</span><br><span class="line">   | or merely testing its existence. Profiles can also be activated by JDK version prefix, where a</span><br><span class="line">   | value of &#39;1.4&#39; might activate a profile when the build is executed on a JDK version of &#39;1.4.2_07&#39;.</span><br><span class="line">   | Finally, the list of active profiles can be specified directly from the command line.</span><br><span class="line">   |</span><br><span class="line">   | NOTE: For profiles defined in the settings.xml, you are restricted to specifying only artifact</span><br><span class="line">   |       repositories, plugin repositories, and free-form properties to be used as configuration</span><br><span class="line">   |       variables for plugins in the POM.</span><br><span class="line">   |</span><br><span class="line">   |--&gt;</span><br><span class="line">  &lt;profiles&gt;</span><br><span class="line">    &lt;!-- profile</span><br><span class="line">     | Specifies a set of introductions to the build process, to be activated using one or more of the</span><br><span class="line">     | mechanisms described above. For inheritance purposes, and to activate profiles via &lt;activatedProfiles&#x2F;&gt;</span><br><span class="line">     | or the command line, profiles have to have an ID that is unique.</span><br><span class="line">     |</span><br><span class="line">     | An encouraged best practice for profile identification is to use a consistent naming convention</span><br><span class="line">     | for profiles, such as &#39;env-dev&#39;, &#39;env-test&#39;, &#39;env-production&#39;, &#39;user-jdcasey&#39;, &#39;user-brett&#39;, etc.</span><br><span class="line">     | This will make it more intuitive to understand what the set of introduced profiles is attempting</span><br><span class="line">     | to accomplish, particularly when you only have a list of profile id&#39;s for debug.</span><br><span class="line">     |</span><br><span class="line">     | This profile example uses the JDK version to trigger activation, and provides a JDK-specific repo.</span><br><span class="line">    &lt;profile&gt;</span><br><span class="line">      &lt;id&gt;jdk-1.4&lt;&#x2F;id&gt;</span><br><span class="line"></span><br><span class="line">      &lt;activation&gt;</span><br><span class="line">        &lt;jdk&gt;1.4&lt;&#x2F;jdk&gt;</span><br><span class="line">      &lt;&#x2F;activation&gt;</span><br><span class="line"></span><br><span class="line">      &lt;repositories&gt;</span><br><span class="line">        &lt;repository&gt;</span><br><span class="line">          &lt;id&gt;jdk14&lt;&#x2F;id&gt;</span><br><span class="line">          &lt;name&gt;Repository for JDK 1.4 builds&lt;&#x2F;name&gt;</span><br><span class="line">          &lt;url&gt;http:&#x2F;&#x2F;www.myhost.com&#x2F;maven&#x2F;jdk14&lt;&#x2F;url&gt;</span><br><span class="line">          &lt;layout&gt;default&lt;&#x2F;layout&gt;</span><br><span class="line">          &lt;snapshotPolicy&gt;always&lt;&#x2F;snapshotPolicy&gt;</span><br><span class="line">        &lt;&#x2F;repository&gt;</span><br><span class="line">      &lt;&#x2F;repositories&gt;</span><br><span class="line">    &lt;&#x2F;profile&gt;</span><br><span class="line">    --&gt;</span><br><span class="line"></span><br><span class="line">    &lt;!--</span><br><span class="line">     | Here is another profile, activated by the system property &#39;target-env&#39; with a value of &#39;dev&#39;,</span><br><span class="line">     | which provides a specific path to the Tomcat instance. To use this, your plugin configuration</span><br><span class="line">     | might hypothetically look like:</span><br><span class="line">     |</span><br><span class="line">     | ...</span><br><span class="line">     | &lt;plugin&gt;</span><br><span class="line">     |   &lt;groupId&gt;org.myco.myplugins&lt;&#x2F;groupId&gt;</span><br><span class="line">     |   &lt;artifactId&gt;myplugin&lt;&#x2F;artifactId&gt;</span><br><span class="line">     |</span><br><span class="line">     |   &lt;configuration&gt;</span><br><span class="line">     |     &lt;tomcatLocation&gt;$&#123;tomcatPath&#125;&lt;&#x2F;tomcatLocation&gt;</span><br><span class="line">     |   &lt;&#x2F;configuration&gt;</span><br><span class="line">     | &lt;&#x2F;plugin&gt;</span><br><span class="line">     | ...</span><br><span class="line">     |</span><br><span class="line">     | NOTE: If you just wanted to inject this configuration whenever someone set &#39;target-env&#39; to</span><br><span class="line">     |       anything, you could just leave off the &lt;value&#x2F;&gt; inside the activation-property.</span><br><span class="line">     |</span><br><span class="line">    &lt;profile&gt;</span><br><span class="line">      &lt;id&gt;env-dev&lt;&#x2F;id&gt;</span><br><span class="line"></span><br><span class="line">      &lt;activation&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">          &lt;name&gt;target-env&lt;&#x2F;name&gt;</span><br><span class="line">          &lt;value&gt;dev&lt;&#x2F;value&gt;</span><br><span class="line">        &lt;&#x2F;property&gt;</span><br><span class="line">      &lt;&#x2F;activation&gt;</span><br><span class="line"></span><br><span class="line">      &lt;properties&gt;</span><br><span class="line">        &lt;tomcatPath&gt;&#x2F;path&#x2F;to&#x2F;tomcat&#x2F;instance&lt;&#x2F;tomcatPath&gt;</span><br><span class="line">      &lt;&#x2F;properties&gt;</span><br><span class="line">    &lt;&#x2F;profile&gt;</span><br><span class="line">    --&gt;</span><br><span class="line">  &lt;&#x2F;profiles&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- activeProfiles</span><br><span class="line">   | List of profiles that are active for all builds.</span><br><span class="line">   |</span><br><span class="line">  &lt;activeProfiles&gt;</span><br><span class="line">    &lt;activeProfile&gt;alwaysActiveProfile&lt;&#x2F;activeProfile&gt;</span><br><span class="line">    &lt;activeProfile&gt;anotherAlwaysActiveProfile&lt;&#x2F;activeProfile&gt;</span><br><span class="line">  &lt;&#x2F;activeProfiles&gt;</span><br><span class="line">  --&gt;</span><br><span class="line">&lt;&#x2F;settings&gt;</span><br></pre></td></tr></table></figure>






</li>
</ul>
</li>
</ol>
<ul>
<li><p>安装findbugs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cd ..&#x2F;usr&#x2F;lib</span><br><span class="line">tar -xvf findbugs-3.0.1.tar.gz</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line">export FINDBUGS_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;findbugs-3.0.1</span><br><span class="line">export PATH:$FINDBUGS_HOME&#x2F;bin:$PATH</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure></li>
<li><p>安装其他依赖</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum install -y autoconf automake libtool cmake make</span><br><span class="line">yum install -y ncurses-devel</span><br><span class="line">yum install -y openssl-devel</span><br><span class="line">yum install -y lzo-devel zlib-devel gcc gcc-c++</span><br><span class="line">yum install -y bzip2-devel</span><br><span class="line">yum install -y snappy</span><br></pre></td></tr></table></figure></li>
<li><p>编译</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mvn package -DskipTests -Pdist,native -Dtar -Drequir.snappy -e -X</span><br></pre></td></tr></table></figure>
<p>Bug0:’protoc –version’ did not return a version</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># 解决</span><br><span class="line"></span><br><span class="line">安装protobuf-2.5.0</span><br><span class="line">https:&#x2F;&#x2F;github.com&#x2F;protocolbuffers&#x2F;protobuf&#x2F;releases&#x2F;download&#x2F;v2.5.0&#x2F;protobuf-2.5.0.tar.gz</span><br><span class="line"></span><br><span class="line">tar -xvf protobuf-2.5.0.tar.gz</span><br><span class="line">cd protobuf-2.5.0</span><br><span class="line"></span><br><span class="line">.&#x2F;configure</span><br><span class="line">make</span><br><span class="line">make check</span><br><span class="line">make install</span><br></pre></td></tr></table></figure>
<p>Bug1:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">[ERROR] Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:2.10.1:cmake-compile (cmake-compile) on project hadoop-pipes: CMake failed with error code 1 -&gt; [Help 1]</span><br><span class="line">org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.hadoop:hadoop-maven-plugins:2.10.1:cmake-compile (cmake-compile) on project hadoop-pipes: CMake failed with error code 1</span><br><span class="line"></span><br><span class="line"># 解决 升级Cmake</span><br><span class="line"></span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;Kitware&#x2F;CMake.git</span><br><span class="line"></span><br><span class="line">cd CMake</span><br><span class="line"></span><br><span class="line">.&#x2F;bootstrap</span><br><span class="line"></span><br><span class="line">make -j8</span><br><span class="line"></span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line">make clean</span><br><span class="line"></span><br><span class="line">ln -s &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cmake &#x2F;usr&#x2F;bin&#x2F;cmake</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 安装这个</span><br><span class="line">yum install libtirpc</span><br><span class="line"></span><br><span class="line">yum install -y automake autoconf gcc-c++ cmake libedit libtool openssl-devel ncurses-devel</span><br></pre></td></tr></table></figure>
<p>Bug2：没找到SNAPPY</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 hadoop-2.10.1]# bin&#x2F;hadoop checknative</span><br><span class="line">20&#x2F;10&#x2F;16 07:56:12 INFO bzip2.Bzip2Factory: Successfully loaded &amp; initialized native-bzip2 library system-native</span><br><span class="line">20&#x2F;10&#x2F;16 07:56:12 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library</span><br><span class="line">Native library checking:</span><br><span class="line">hadoop:  true &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;lib&#x2F;native&#x2F;libhadoop.so.1.0.0</span><br><span class="line">zlib:    true &#x2F;lib64&#x2F;libz.so.1</span><br><span class="line">snappy:  false </span><br><span class="line">zstd  :  false </span><br><span class="line">lz4:     true revision:10301</span><br><span class="line">bzip2:   true &#x2F;lib64&#x2F;libbz2.so.1</span><br><span class="line">openssl: true &#x2F;lib64&#x2F;libcrypto.so</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 解决 不能用yum安装</span><br><span class="line">yum remove snappy* -y</span><br><span class="line"></span><br><span class="line">tar -xvf snappy-1.1.8.tar.gz</span><br><span class="line">cd snappy-1.1.8</span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 换版本，1.1.3</span><br><span class="line">重新编译,还是不行，</span><br><span class="line">把变异好的snappy文件 放入hadoop-2.10.1&#x2F;lib里</span><br><span class="line">成功</span><br></pre></td></tr></table></figure>
<p>重新编译</p>
<p>测试</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd hadoop-2.10.1</span><br><span class="line">bin&#x2F;hadoop checknative</span><br></pre></td></tr></table></figure>
<h3 id="4、修改hadoop配置文件"><a href="#4、修改hadoop配置文件" class="headerlink" title="4、修改hadoop配置文件"></a>4、修改hadoop配置文件</h3></li>
</ul>
<p>第一台机器执行</p>
<ol>
<li><p>core-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 定义hadoop集群文件系统类型 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.default.name&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;hdfs:&#x2F;&#x2F;192.168.250.110:8020&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- hadoop文件存储临时目录 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;tempDatas&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- hadoop缓冲区大小 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;io.file.buffer.size&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;4096&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 开启HDFS的垃圾桶机制，删除掉的数据可以从垃圾桶回收，单位分钟 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.trash.interval&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;10080&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="2">
<li><p>Hdfs-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!--  指定namenode辅助节点地址--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.secondary.http-address&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;192.168.250.110:50090&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;!--  指定namenode访问节点地址--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.http-address&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;192.168.250.110:50070&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;!--  指定namenode存储元数据位置--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.name.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;namenodeDatas,file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;namenodeDatas2&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">&lt;!--  指定datanode存储数据节点位置，实际工作中，一般先确定磁盘的挂在目录，然后多个目录用“，”进行分割--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.datanode.data.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;datanodeDatas，file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;datanodeDatas2&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--  指定namenode日志文件存放地址--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.edits.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;nn&#x2F;edits&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--  指定namenode检查点存放地址--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.checkpoint.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;snn&#x2F;name&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!--  指定namenode检查点日志文件存放地址--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.namenode.checkpoint.edits.dir&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;file:&#x2F;&#x2F;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;dfs&#x2F;snn&#x2F;edits&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!--指定一个文件存储的副本个数（文件切片的副本个数）--&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;3&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!-- 设置HDFS的文件权限 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.permissions&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;false&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!-- 指定一个文件切片的大小（128M） --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.blocksize&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;134217728&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="3">
<li><p>Hadoop-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jdk1.8.0_261</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="4">
<li><p>Mapred-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line">&lt;?xml-stylesheet type&#x3D;&quot;text&#x2F;xsl&quot; href&#x3D;&quot;configuration.xsl&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">  &lt;!-- 开启MapReduce小任务模式 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.job.ubertask.enable&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!-- 设置历史任务的主机和端口 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.address&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;192.168.250.110:10020&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!-- 设置历史任务网页APP的主机和端口 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.jobhistory.webapp.ddress&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;192.168.250.110:19888&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="5">
<li><p>Yarn-site.xml</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot;?&gt;</span><br><span class="line"></span><br><span class="line">&lt;configuration&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- Site specific YARN configuration properties --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 配置yarn主节点的位置 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;node01&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--  --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!-- 开启日志聚合功能 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation-enable&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!-- 聚合日志在hdfs上保存的时间 （s） --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.log-aggregation.retain-senconds&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;604800&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">  &lt;!-- 下面三个设置yarn集群的内存分配方案 --&gt;</span><br><span class="line">  &lt;!--  --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.resource.memory-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;20480&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!--  --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.scheduler.minimum-allocation-mb&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;2048&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property&gt;</span><br><span class="line">  &lt;!--  --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.vmem-pmem-ratio&lt;&#x2F;name&gt;</span><br><span class="line">    &lt;value&gt;2.1&lt;&#x2F;value&gt;</span><br><span class="line">  &lt;&#x2F;property</span><br><span class="line"></span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="6">
<li><p>Mapred-env.sh</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;jdk1.8.0_261</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="7">
<li><p>Slaves</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node01</span><br><span class="line">node02</span><br><span class="line">node03</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="5、配置hadoop环境变量"><a href="#5、配置hadoop环境变量" class="headerlink" title="5、配置hadoop环境变量"></a>5、配置hadoop环境变量</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;tempDatas</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;namenodeDatas</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;namenodeDatas2</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;datanodeDatas</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;datanodeDatas2</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;nn&#x2F;edits</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;snn&#x2F;name</span><br><span class="line">mkdir -p &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;hadoopDatas&#x2F;dfs&#x2F;snn&#x2F;edits</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 拷贝文件</span><br><span class="line">cd &#x2F;usr&#x2F;lib</span><br><span class="line">scp -r hadoop-2.10.1 node02:$PWD</span><br><span class="line">scp -r hadoop-2.10.1 node03:$PWD</span><br><span class="line"></span><br><span class="line"># 三台机器执行</span><br><span class="line"># 配置环境变量</span><br><span class="line">vi &#x2F;etc&#x2F;profile</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1</span><br><span class="line">export PATH:$HADOOP_HOME&#x2F;bin:$HADOOP_HOME&#x2F;sbin:$PATH</span><br><span class="line">source &#x2F;etc&#x2F;profile</span><br></pre></td></tr></table></figure>
<h3 id="6、启动hadoop集群"><a href="#6、启动hadoop集群" class="headerlink" title="6、启动hadoop集群"></a>6、启动hadoop集群</h3><p>启动hadoop集群需要启动hdfs和yarn两个模块。首次启动hdfs必须对其进行格式化操作。</p>
<p>hdfs namenode -format 或者 hadoop namenode -format</p>
<p>第一台机器执行</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1</span><br><span class="line">bin&#x2F;hdfs namenode -format</span><br><span class="line">sbin&#x2F;start-dfs.sh</span><br><span class="line">sbin&#x2F;start-yarn.sh</span><br><span class="line">sbin&#x2F;mr-jobhistory-daemon.sh start historyserver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">jps</span><br><span class="line">[root@node01 hadoop-2.10.1]# jps</span><br><span class="line">6818 SecondaryNameNode</span><br><span class="line">7653 ResourceManager</span><br><span class="line">6233 NameNode</span><br><span class="line">6475 DataNode</span><br><span class="line">7821 NodeManager</span><br><span class="line">9869 JobHistoryServer</span><br><span class="line">9966 Jps</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/felixzh/p/12069843.html">https://www.cnblogs.com/felixzh/p/12069843.html</a></p>
<h2 id="四、HDFS（hadoop-distributed-file-system）"><a href="#四、HDFS（hadoop-distributed-file-system）" class="headerlink" title="四、HDFS（hadoop distributed file system）"></a>四、HDFS（hadoop distributed file system）</h2><p>HDFS使用多台计算机存储文件，并提供统一的访问接口，像访问一个普通文件系统一样使用分布式文件系统。</p>
<h3 id="1、HDFS应用场景"><a href="#1、HDFS应用场景" class="headerlink" title="1、HDFS应用场景"></a>1、HDFS应用场景</h3><ol>
<li>适合的场景<ul>
<li>存储非常大的文件，需要高吞吐量，无延时要求</li>
<li>采用流式的数据访问方式：<strong>即一次写入，多次读取</strong>，数据经常从数据源生成和拷贝（适合做数据分析，不适合网盘）</li>
<li>运行在商业硬件上，节约成本，可在廉价机器上运行</li>
<li>需要高容错性（多个副本）</li>
<li>为数据存储提供拓展能力（对集群进行拓展）</li>
</ul>
</li>
<li>不适合的场景<ul>
<li>低延时的数据访问，如毫秒级</li>
<li>不适合存储大量的小文件，因为文件的元数据存储在namenode的内存中，整个文件系统的文件数量受限于namenode的内存大小，因此大量小文件不适用</li>
<li>多方读写，HDFS采用追加append的方式写入数据。不支持文件任意offset的修改，不支持多个写入器。（不能随意定位到文件的某个位置进行修改）不支持并发写入，只能一个写，不能多线程同时写。</li>
</ul>
</li>
</ol>
<h3 id="2、HDFS组成"><a href="#2、HDFS组成" class="headerlink" title="2、HDFS组成"></a>2、HDFS组成</h3><ol>
<li><p>HDFS client</p>
<ul>
<li>文件切分 切片，将文件切分成一个一个的block，然后进行存储，<strong>hdfs块大小设置主要取决于磁盘传输速率。</strong></li>
<li>与namenode交互，获取文件的位置信息</li>
<li>与datanode交互，读取或者写入信息</li>
<li>client提供一些命令来管理和访问hdfs，如启动和关闭hdfs</li>
<li>client通过一些命令来访问hdfs，增删查改</li>
</ul>
</li>
<li><p>namenode</p>
<ul>
<li>管理HDFS的名称空间</li>
<li>管理block的映射信息</li>
<li>配置文件副本策略</li>
<li>处理客户端读写请求</li>
</ul>
</li>
<li><p>datanode</p>
<p>就是slave，namenode下达命令，datanode执行实际的操作</p>
<ul>
<li>存储实际的数据块</li>
<li>执行数据块的读、写操作</li>
</ul>
</li>
<li><p>secondary namenode</p>
<p>secondary namenode不是namenode的热备份，当namenode挂掉，它并不能马上替换掉namenode。</p>
<ul>
<li>辅助namenode，分担工作量</li>
<li>定期合并fsimage和fsedits，并推送给namenode</li>
<li>在紧急情况下可辅助恢复namenode</li>
</ul>
</li>
</ol>
<h3 id="3、namenode"><a href="#3、namenode" class="headerlink" title="3、namenode"></a>3、namenode</h3><p>存储：</p>
<ul>
<li>在内存中保存整个文件系统的<strong>名称空间</strong>和数据块的<strong>地址映射</strong></li>
<li>整个hdfs可存储的文件数量受限于namenode的内存大小</li>
</ul>
<ol>
<li><p>namenode元数据信息</p>
<p>文件名、文件目录结构、文件属性（生成时间、副本数、权限等），每个文件的块列表、列表中块与快所在的datanode之间的地址映射关系。</p>
<p>在内存中家在文件系统中每个文件和每个数据块的引用关系（文件，block，datanode映射关系）数据回定期保存到本地磁盘（fsimage、edits）</p>
</li>
<li><p>namenode文件操作</p>
<p>namenode负责文件元数据的操作，负责处理文件内容读写请求，数据流不经过datanode</p>
</li>
<li><p>namenode副本</p>
<p>namenode决定文件数据块存放到哪些datanode上，根据全局情况作出放置副本的决定</p>
</li>
<li><p>namenode心跳机制</p>
<p>全权管理数据块的复制，周期性接受心跳和块的状态报告，</p>
</li>
</ol>
<h3 id="4、DataNode"><a href="#4、DataNode" class="headerlink" title="4、DataNode"></a>4、DataNode</h3><ol>
<li>以数据块形式存储HDFS文件</li>
<li>响应HDFS客户端读写请求</li>
<li>周期性向namenode汇报心跳信息、数据块信息、缓存数据块信息</li>
</ol>
<h3 id="5、HDFS的副本机制和机架感知"><a href="#5、HDFS的副本机制和机架感知" class="headerlink" title="5、HDFS的副本机制和机架感知"></a>5、HDFS的副本机制和机架感知</h3><ol>
<li><p>副本机制</p>
<p>所有文件都是以block块的方式存放在HDFS文件系统中的，block块大小可以通过hdfs-site.xml中的dfs.block.size调节</p>
</li>
<li><p>机架感知</p>
<p>系统默认文件3个副本，其中一个副本存放在本机架（同局域网），另一个副本存放在外机架</p>
</li>
</ol>
<h3 id="6、HDFS的shell基本命令"><a href="#6、HDFS的shell基本命令" class="headerlink" title="6、HDFS的shell基本命令"></a>6、HDFS的shell基本命令</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"># ls</span><br><span class="line">hdfs dfs -ls URL</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# hdfs dfs -ls &#x2F;</span><br><span class="line">Found 1 items</span><br><span class="line">drwxrwx---   - root supergroup          0 2020-10-18 04:18 &#x2F;tmp</span><br><span class="line"></span><br><span class="line"># lsr 递归执行ls，包含子目录</span><br><span class="line">hdfs dfs -ls -R URL</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# hdfs dfs -ls -R &#x2F;</span><br><span class="line">drwxrwx---   - root supergroup          0 2020-10-18 04:18 &#x2F;tmp</span><br><span class="line">drwxrwx---   - root supergroup          0 2020-10-18 04:18 &#x2F;tmp&#x2F;hadoop-yarn</span><br><span class="line">drwxrwx---   - root supergroup          0 2020-10-18 04:18 &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging</span><br><span class="line">drwxrwx---   - root supergroup          0 2020-10-18 04:18 &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;history</span><br><span class="line">drwxrwx---   - root supergroup          0 2020-10-18 04:18 &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;history&#x2F;done</span><br><span class="line">drwxrwxrwt   - root supergroup          0 2020-10-18 04:18 &#x2F;tmp&#x2F;hadoop-yarn&#x2F;staging&#x2F;history&#x2F;done_intermediate</span><br><span class="line"></span><br><span class="line"># mkdir</span><br><span class="line">hdfs dfs [-p] -mkdir &lt;PATH&gt;</span><br><span class="line">#非递归</span><br><span class="line">[root@node01 ~]# hdfs dfs -mkdir &#x2F;dir1</span><br><span class="line">[root@node01 ~]# hdfs dfs -ls &#x2F;</span><br><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2020-10-18 08:37 &#x2F;dir1</span><br><span class="line">drwxrwx---   - root supergroup          0 2020-10-18 04:18 &#x2F;tmp</span><br><span class="line">#递归创建，&#x2F;dir2不存在</span><br><span class="line">hdfs dfs -mkdir -p &#x2F;dir2&#x2F;dir22</span><br><span class="line">[root@node01 ~]# hdfs dfs -mkdir -p &#x2F;dir2&#x2F;dir22</span><br><span class="line">[root@node01 ~]# hdfs dfs -ls &#x2F;dir2</span><br><span class="line">Found 1 items</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2020-10-18 08:40 &#x2F;dir2&#x2F;dir22</span><br><span class="line"></span><br><span class="line"># put 将单个的源文件src或者多个源文件srcs从本地文件系统拷贝到目标HDFS文件系统；也可以从标准输入中读取输入，写入目标文件系统中</span><br><span class="line">hdfs dfs -put &lt;source&gt; &lt;target&gt;s</span><br><span class="line">[root@node01 ~]# hdfs dfs -put a.txt &#x2F;dir1</span><br><span class="line">[root@node01 ~]# hdfs dfs -ls &#x2F;dir1</span><br><span class="line">Found 1 items</span><br><span class="line">-rw-r--r--   3 root supergroup         20 2020-10-18 08:42 &#x2F;dir1&#x2F;a.txt</span><br><span class="line"></span><br><span class="line">#moveFromLocal 与put类似，但是源文件在操作后自身被删除</span><br><span class="line">hdfs dfs -moveFromLocal &lt;local&gt; &lt;target&gt;</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# touch 1.txt</span><br><span class="line">[root@node01 ~]# hdfs dfs -moveFromLocal 1.txt &#x2F;dir1</span><br><span class="line">[root@node01 ~]# ls</span><br><span class="line"> anaconda-ks.cfg   a.txt  &#39;udo systemctl enable --now mysqld&#39;</span><br><span class="line"></span><br><span class="line"># moveToLocal</span><br><span class="line"></span><br><span class="line"># get 将文件拷贝到本地文件系统，-ignorecrc  -crc代表文件校验</span><br><span class="line">hdfs dfs -get [-ignorecrc]  [-crc] &lt;src&gt; &lt;local&gt;</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# hdfs dfs -get &#x2F;dir1&#x2F;1.txt ~</span><br><span class="line">[root@node01 ~]# ls</span><br><span class="line"> 1.txt   anaconda-ks.cfg   a.txt  &#39;udo systemctl enable --now mysqld&#39;</span><br><span class="line"></span><br><span class="line"># mv  将hdfs上的文件移动到目标路径（移动后源文件删除），不能跨文件系统</span><br><span class="line">hdfs dfs -mv URI &lt;target&gt;</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# hdfs dfs -mv &#x2F;dir1&#x2F;a.txt &#x2F;dir2</span><br><span class="line">[root@node01 ~]# hdfs dfs -ls  &#x2F;dir2</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup         20 2020-10-18 08:42 &#x2F;dir2&#x2F;a.txt</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2020-10-18 08:40 &#x2F;dir2&#x2F;dir22</span><br><span class="line"></span><br><span class="line"># rm 删除 [-r]递归删除 [-skipTrash]跳过回收站直接删除 可以删除多个文件，只能删除文件和非空目录，如果回收站可用，则文件将暂时存放到回收站中</span><br><span class="line"></span><br><span class="line">hdfs dfs -rm &#x2F;dir2&#x2F;a.txt</span><br><span class="line">[root@node01 ~]# hdfs dfs -rm &#x2F;dir2&#x2F;a.txt</span><br><span class="line">20&#x2F;10&#x2F;18 09:06:10 INFO fs.TrashPolicyDefault: Moved: &#39;hdfs:&#x2F;&#x2F;192.168.250.110:8020&#x2F;dir2&#x2F;a.txt&#39; to trash at: hdfs:&#x2F;&#x2F;192.168.250.110:8020&#x2F;user&#x2F;root&#x2F;.Trash&#x2F;Current&#x2F;dir2&#x2F;a.txt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># cp 复制文件到指定路径 [-f]选项将覆盖目标 [-p] 深度拷贝，保留文件属性</span><br><span class="line">hdfs dfs -cp URI &lt;target&gt;</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# hdfs dfs -cp &#x2F;dir1&#x2F;1.txt &#x2F;dir2</span><br><span class="line">[root@node01 ~]# hdfs dfs -ls &#x2F;dir2</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2020-10-18 09:11 &#x2F;dir2&#x2F;1.txt</span><br><span class="line">drwxr-xr-x   - root supergroup          0 2020-10-18 08:40 &#x2F;dir2&#x2F;dir22</span><br><span class="line"></span><br><span class="line"># cat 显示文件内容</span><br><span class="line">hdfs dfs -cat URI</span><br><span class="line"></span><br><span class="line"># chmod 改变文件权限 [-R]对整个目录有效递归执行</span><br><span class="line">hdfs dfs -chmod [-R] URI</span><br><span class="line">hdfs dfs -chmod 777 &#x2F;dir1&#x2F;1.txt</span><br><span class="line"></span><br><span class="line"># chown 改变文件所属用户和用户组 [-R]对整个目录有效递归执行</span><br><span class="line">hdfs dfs -chown [-R] URI</span><br><span class="line"></span><br><span class="line"># appendToFile 追加一个或多个本地文件到HDFS文件</span><br><span class="line">hdfs dfs -appendToFile &lt;local&gt; &lt;src&gt;</span><br></pre></td></tr></table></figure>
<h3 id="7、HDFS高级命令"><a href="#7、HDFS高级命令" class="headerlink" title="7、HDFS高级命令"></a>7、HDFS高级命令</h3><ol>
<li><p>HDFS文件限额配置</p>
<p>对每个用户仅操作一个目录，然后对目录设置配置。HDFS文件的限额配置以<strong>文件个数</strong>或者<strong>文件大小</strong>来限制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -count -q -h &lt;path&gt;</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# hdfs dfs -mkdir &#x2F;user&#x2F;root&#x2F;dir</span><br><span class="line">[root@node01 ~]# hdfs dfs -count -q -h &#x2F;user&#x2F;root&#x2F;dir</span><br><span class="line">        none(文件个数限额)             inf            none(文件大小限额)             inf            1            0                  0 &#x2F;user&#x2F;root&#x2F;dir</span><br></pre></td></tr></table></figure>
<p>文件数量限额</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -setQuota filenum path</span><br><span class="line"></span><br><span class="line">#设置文件个数配额</span><br><span class="line">hdfs dfsadmin -setQuota 2 dir</span><br><span class="line"># 当给某个目录设置n个文件限额，其实只能上传n-1个</span><br><span class="line">[root@node01 ~]# hdfs dfsadmin -setQuota 2 dir</span><br><span class="line">[root@node01 ~]# hdfs dfs -count -q -h &#x2F;user&#x2F;root&#x2F;dir</span><br><span class="line">           2               1(剩余文件限额个数)            none             inf            1            0                  0 &#x2F;user&#x2F;root&#x2F;dir</span><br><span class="line"></span><br><span class="line">[root@node01 ~]# hdfs dfs -put a.txt dir</span><br><span class="line">[root@node01 ~]# hdfs dfs -count -q -h &#x2F;user&#x2F;root&#x2F;dir</span><br><span class="line">           2               0(剩余文件限额个数)            none             inf            1            1                 20 &#x2F;user&#x2F;root&#x2F;dir</span><br><span class="line"></span><br><span class="line">#清除文件配额限制</span><br><span class="line">hdfs dfsadmin -clrQuota &#x2F;user&#x2F;root&#x2F;dir</span><br></pre></td></tr></table></figure>
<p>空间大小限额配置</p>
<p>设置的空间<strong>至少</strong>是blocksize*3的大小</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -setSpaceQuota 384M path</span><br></pre></td></tr></table></figure>
<p>清除空间配额限制</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -clrSpaceQuota path</span><br></pre></td></tr></table></figure>
<p>生成任意大小文件的命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dd if&#x3D;&#x2F;dev&#x2F;zero of&#x3D;1.txt bs&#x3D;1M count&#x3D;2</span><br></pre></td></tr></table></figure></li>
<li><p>hdfs的安全模式</p>
<p>安全模式是hadoop的一种保护机制，保证集群中数据块的安全性。当集群启动时候，会首先进入安全模式，检查数据块的完整性。</p>
<p>副本率：hadoop默认副本率为0.999，如果设置副本数为3，那么在datanode上就应该存在3个副本，假设只存在2个，那么比利就是2/3=0.666，检查到副本率低于0.999，则会自动复制副本到datanode使副本率不低于默认值；如果副本超过设定副本个数，则删除多余的副本。</p>
<p>在安全模式下，文件系统只支持读数据，不支持删除修改，当达到安全标准时，hdfs自动离开安全模式。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfsadmin -safemode get #查看安全模式状态</span><br><span class="line">hdfs dfsadmin -safemode enter #进入</span><br><span class="line">hdfs dfsadmin -safemode leave #离开</span><br></pre></td></tr></table></figure>
<h3 id="8、hdfs基准测试"><a href="#8、hdfs基准测试" class="headerlink" title="8、hdfs基准测试"></a>8、hdfs基准测试</h3></li>
<li><p>测试写入速度(写十个文件，每个10MB，存放在benchmark/TestDFSIO)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-client-jobclient-2.10.1.jar TestDFSIO -write -nrFiles 10 -fileSize 10MB</span><br></pre></td></tr></table></figure>
<p>查看写入速度结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -text &#x2F;benchmarks&#x2F;TestDFSIO&#x2F;io_write&#x2F;part-00000</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="2">
<li><p>测试读取速度（读10个文件，每个10M)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-client-jobclient-2.10.1.jar TestDFSIO -read -nrFiles 10 -fileSize 10MB</span><br></pre></td></tr></table></figure>
<p>查看写入速度结果</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -text &#x2F;benchmarks&#x2F;TestDFSIO&#x2F;io_read&#x2F;part-00000</span><br></pre></td></tr></table></figure></li>
<li><p>清楚测试数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar &#x2F;usr&#x2F;lib&#x2F;hadoop-2.10.1&#x2F;share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-client-jobclient-2.10.1.jar TestDFSIO -clean</span><br></pre></td></tr></table></figure>
<h3 id="9、hdfs文件读写过程"><a href="#9、hdfs文件读写过程" class="headerlink" title="9、hdfs文件读写过程"></a>9、hdfs文件读写过程</h3></li>
</ol>
<p><img src="file:///Users/lankangning/Desktop/工作/image-20201019150624206.png?lastModify=1609225369" alt="image-20201019150624206"></p>
<p><img src="file:///Users/lankangning/Desktop/工作/20170904183834049.png?lastModify=1609225369" alt="20170904183834049"></p>
<p><img src="file:///Users/lankangning/Desktop/工作/image-20201019150650566.png?lastModify=1609225369" alt="image-20201019150650566"></p>
<p><img src="file:///Users/lankangning/Desktop/工作/20170904184135696.png?lastModify=1609225369" alt="20170904184135696"></p>
<h3 id="10、HDFS的元数据管理"><a href="#10、HDFS的元数据管理" class="headerlink" title="10、HDFS的元数据管理"></a>10、HDFS的元数据管理</h3><h3 id="11、HDFS的secondaryNamenode机制"><a href="#11、HDFS的secondaryNamenode机制" class="headerlink" title="11、HDFS的secondaryNamenode机制"></a>11、HDFS的secondaryNamenode机制</h3><h3 id="12、HDFS的JavaAPI"><a href="#12、HDFS的JavaAPI" class="headerlink" title="12、HDFS的JavaAPI"></a>12、HDFS的JavaAPI</h3><ol>
<li><p>URL方式访问数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">    使用url方式访问数据（了解）</span><br><span class="line">*&#x2F;</span><br><span class="line">@Test</span><br><span class="line">public void demo1() throws Exception&#123;</span><br><span class="line">	&#x2F;&#x2F;注册hdfs的url</span><br><span class="line">  URL.setURLStreamHandlerFactory(new FsUrlStreamHandlerFactory());</span><br><span class="line">  &#x2F;&#x2F; 注意是文件</span><br><span class="line">  InputStream inputStream &#x3D; new URL(&quot;hdfs:&#x2F;&#x2F;node01:8020&#x2F;a.txt&quot;).openStream();</span><br><span class="line">  FileOutputStream outputStream &#x3D; new FileOutputStream(new File(&quot;&#x2F;Users&#x2F;lankangning&#x2F;desktop&#x2F;a.txt&quot;));</span><br><span class="line">  &#x2F;&#x2F;拷贝文件</span><br><span class="line">  IOUtils.copy(inputStream,outputStream);</span><br><span class="line">  &#x2F;&#x2F;关闭流</span><br><span class="line">  IOUtils.closeQuietly(inputStream);</span><br><span class="line">  IOUtils.closeQuietly(outputStream);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>使用文件系统方式访问数据</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; configuration类  该类封装了客户端或者服务器的配置</span><br><span class="line">&#x2F;&#x2F;FileSystem类	该类的对象是一个文件系统对象，通过对象的一些方法对文件进行操作</span><br><span class="line">FileSystem fs &#x3D; FileSystem.get(conf)</span><br></pre></td></tr></table></figure>
<p>获取FileSystem的几种方法</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">    获取Filesystem</span><br><span class="line">     *&#x2F;</span><br><span class="line">@Test</span><br><span class="line">public void getFileSystem1() throws Exception&#123;</span><br><span class="line">  &#x2F;&#x2F;创建conf对象</span><br><span class="line">    Configuration configuration &#x3D; new Configuration();</span><br><span class="line">    &#x2F;&#x2F;设置文件系统类型 parm0:文件系统的类型，parm1：哪一个种文件系统 hdfs:&#x2F;&#x2F;;file:&#x2F;&#x2F;&#x2F;</span><br><span class="line">    configuration.set(&quot;fs.defaultFS&quot;,&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;);</span><br><span class="line">    &#x2F;&#x2F;获取指定的文件系统</span><br><span class="line">    FileSystem fs &#x3D; FileSystem.get(configuration);</span><br><span class="line">    System.out.println(fs.toString());</span><br><span class="line">&#125;</span><br><span class="line">@Test &#x2F;&#x2F; 常用</span><br><span class="line">public void getFileSystem2() throws Exception&#123;</span><br><span class="line">    FileSystem fs &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;),new Configuration());</span><br><span class="line">    System.out.println(fs.toString());</span><br><span class="line">&#125;</span><br><span class="line">@Test</span><br><span class="line">public void getFileSystem3() throws Exception&#123;</span><br><span class="line">    Configuration configuration &#x3D; new Configuration();</span><br><span class="line">    &#x2F;&#x2F;设置文件系统类型 parm0:文件系统的类型，parm1：哪一个种文件系统 hdfs:&#x2F;&#x2F;;file:&#x2F;&#x2F;&#x2F;</span><br><span class="line">    configuration.set(&quot;fs.defaultFS&quot;,&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;);</span><br><span class="line">    &#x2F;&#x2F;获取指定的文件系统</span><br><span class="line">    FileSystem fs &#x3D; FileSystem.newInstance(configuration);</span><br><span class="line">    System.out.println(fs.toString());</span><br><span class="line">&#125;</span><br><span class="line">@Test</span><br><span class="line">public void getFileSystem4() throws Exception&#123;</span><br><span class="line">    FileSystem fs &#x3D; FileSystem.newInstance(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;),new Configuration());</span><br><span class="line">    System.out.println(fs.toString());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>遍历hdfs文件</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">    遍历HDFS文件</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Test</span><br><span class="line">    public void listFiles() throws Exception&#123;</span><br><span class="line">        &#x2F;&#x2F;获取filesystem实例</span><br><span class="line">        FileSystem fileSystem &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;),new Configuration());</span><br><span class="line">        &#x2F;&#x2F;调用listFiles方法,返回一个迭代器</span><br><span class="line">        RemoteIterator&lt;LocatedFileStatus&gt; iterator &#x3D; fileSystem.listFiles(new Path(&quot;&#x2F;&quot;), true);</span><br><span class="line">        &#x2F;&#x2F;遍历迭代器</span><br><span class="line">        while(iterator.hasNext()) &#123;</span><br><span class="line">            &#x2F;&#x2F;获取文件信息</span><br><span class="line">            LocatedFileStatus fileStatus &#x3D; iterator.next();</span><br><span class="line">            &#x2F;&#x2F;获取文件的绝对路径</span><br><span class="line">            Path filepath &#x3D; fileStatus.getPath();</span><br><span class="line">            &#x2F;&#x2F;获取文件名字</span><br><span class="line">            String filename &#x3D; filepath.getName();</span><br><span class="line">            System.out.println(filepath + &quot;————&quot; + filename);</span><br><span class="line">            &#x2F;&#x2F;获取文件block信息</span><br><span class="line">            BlockLocation[] blockLocations &#x3D; fileStatus.getBlockLocations();</span><br><span class="line">            System.out.println(&quot;block数&quot; + blockLocations.length);</span><br><span class="line">        &#125;</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<p>创建文件/文件夹</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">    hdfs上创建文件夹</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Test</span><br><span class="line">    public void hdfsmkdir() throws Exception&#123;</span><br><span class="line">        &#x2F;&#x2F;获取filesystem实例</span><br><span class="line">        FileSystem fileSystem &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;),new Configuration());</span><br><span class="line">        &#x2F;&#x2F;调用mkdirs方法</span><br><span class="line">        fileSystem.mkdirs(new Path(&quot;&#x2F;hello&#x2F;hello1&quot;));</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;*</span><br><span class="line">    hdfs上创建文件</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Test</span><br><span class="line">    public void hdfscreatefiles() throws Exception&#123;</span><br><span class="line">        &#x2F;&#x2F;获取filesystem实例</span><br><span class="line">        FileSystem fileSystem &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;),new Configuration());</span><br><span class="line">        &#x2F;&#x2F;调用mkdirs方法</span><br><span class="line">        fileSystem.create(new Path(&quot;&#x2F;hello&#x2F;hello1&#x2F;hello.txt&quot;));</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure></li>
<li><p>文件下载/上传</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;*</span><br><span class="line">    hdfs的文件下载</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Test</span><br><span class="line">    public void getFileToLocal() throws Exception&#123;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;获取filesystem实例</span><br><span class="line">        FileSystem fileSystem &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;),new Configuration());</span><br><span class="line">        &#x2F;&#x2F; copy方法</span><br><span class="line">&#x2F;&#x2F;        InputStream inputStream &#x3D; fileSystem.open(new Path(&quot;&#x2F;a.txt&quot;));</span><br><span class="line">&#x2F;&#x2F;        FileOutputStream outputStream &#x3D; new FileOutputStream(new File(&quot;&#x2F;Users&#x2F;lankangning&#x2F;desktop&#x2F;a2.txt&quot;));</span><br><span class="line">&#x2F;&#x2F;        IOUtils.copy(inputStream,outputStream);</span><br><span class="line">&#x2F;&#x2F;        IOUtils.closeQuietly(inputStream);</span><br><span class="line">&#x2F;&#x2F;        IOUtils.closeQuietly(outputStream);</span><br><span class="line">&#x2F;&#x2F;        fileSystem.close();</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;调用copytolocalfile方法</span><br><span class="line">        fileSystem.copyToLocalFile(new Path(&quot;&#x2F;a.txt&quot;),new Path(&quot;&#x2F;Users&#x2F;lankangning&#x2F;desktop&#x2F;a3.txt&quot;));</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;*</span><br><span class="line">    hdfs的文件上传</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Test</span><br><span class="line">    public void putFileFromLocal() throws Exception&#123;</span><br><span class="line"></span><br><span class="line">        &#x2F;&#x2F;获取filesystem实例</span><br><span class="line">        FileSystem fileSystem &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;),new Configuration());</span><br><span class="line">        &#x2F;&#x2F; 注意是文件</span><br><span class="line">&#x2F;&#x2F;        FileInputStream inputStream &#x3D; new FileInputStream(new File(&quot;&#x2F;Users&#x2F;lankangning&#x2F;desktop&#x2F;a2.txt&quot;));</span><br><span class="line">&#x2F;&#x2F;        OutputStream outputStream &#x3D; fileSystem.append(new Path(&quot;&#x2F;a.txt&quot;));</span><br><span class="line">&#x2F;&#x2F;        IOUtils.copy(inputStream,outputStream);</span><br><span class="line">&#x2F;&#x2F;        IOUtils.closeQuietly(inputStream);</span><br><span class="line">&#x2F;&#x2F;        IOUtils.closeQuietly(outputStream);</span><br><span class="line">&#x2F;&#x2F;        fileSystem.close();</span><br><span class="line">        fileSystem.copyFromLocalFile(new Path(&quot;&#x2F;Users&#x2F;lankangning&#x2F;desktop&#x2F;a3.txt&quot;),new Path(&quot;&#x2F;hello&#x2F;a.txt&quot;));</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="13、HDFS访问权限控制"><a href="#13、HDFS访问权限控制" class="headerlink" title="13、HDFS访问权限控制"></a>13、HDFS访问权限控制</h3></li>
<li><p>停止hdfs集群</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sbin&#x2F;stop-dfs.sh</span><br></pre></td></tr></table></figure>
<ol>
<li>修改hdfs-site.xml的参数</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 设置HDFS的文件权限 --&gt;</span><br><span class="line">&lt;!-- 开起权限控制 --&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;dfs.permissions&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;value&gt;true&lt;&#x2F;value&gt;</span><br><span class="line">&lt;&#x2F;property&gt;</span><br><span class="line"></span><br><span class="line">在得不到权限的时候可以伪装成 某个用户来访问 </span><br><span class="line">FileSystem fileSystem &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;),new Configuration(),&quot;root&quot;);</span><br></pre></td></tr></table></figure>
<h3 id="14、hdfs小文件合并"><a href="#14、hdfs小文件合并" class="headerlink" title="14、hdfs小文件合并"></a>14、hdfs小文件合并</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 将hdfs小文件合并，下载到本地</span><br><span class="line">hdfs dfs -getmerge &#x2F;config&#x2F;*.xml &#x2F;User&#x2F;lankangning&#x2F;desktop&#x2F;hello.xml</span><br></pre></td></tr></table></figure>


<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&#x2F; 将本地小文件合并，上传到hdfs</span><br><span class="line">&#x2F;*</span><br><span class="line">    合并本地小文件，上传到hdfs</span><br><span class="line">     *&#x2F;</span><br><span class="line">    @Test</span><br><span class="line">    public void mergeFile() throws Exception &#123;</span><br><span class="line">        &#x2F;&#x2F;获取filesystem实例</span><br><span class="line">        FileSystem fileSystem &#x3D; FileSystem.get(new URI(&quot;hdfs:&#x2F;&#x2F;node01:8020&quot;), new Configuration(),&quot;root&quot;);</span><br><span class="line">        &#x2F;&#x2F;获取hdfs的大文件输出流</span><br><span class="line">        FSDataOutputStream outputStream &#x3D; fileSystem.create(new Path(&quot;&#x2F;bigfile.txt&quot;));</span><br><span class="line">        &#x2F;&#x2F;获取本地文件系统</span><br><span class="line">        LocalFileSystem localFileSystem &#x3D; FileSystem.getLocal(new Configuration());</span><br><span class="line">        &#x2F;&#x2F;获取本地文件夹下所有文件</span><br><span class="line">        FileStatus[] fileStatuses &#x3D; localFileSystem.listStatus(new Path(&quot;file:&#x2F;&#x2F;&#x2F;Users&#x2F;lankangning&#x2F;desktop&#x2F;txt&quot;));</span><br><span class="line">        for(FileStatus fileStatus:fileStatuses)&#123;</span><br><span class="line">            FSDataInputStream inputStream &#x3D; localFileSystem.open(fileStatus.getPath());</span><br><span class="line">            IOUtils.copy(inputStream,outputStream);</span><br><span class="line">            IOUtils.closeQuietly(inputStream);</span><br><span class="line">        &#125;</span><br><span class="line">        IOUtils.closeQuietly(outputStream);</span><br><span class="line">        localFileSystem.close();</span><br><span class="line">        fileSystem.close();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h3 id="15、hdfs高可用机制"><a href="#15、hdfs高可用机制" class="headerlink" title="15、hdfs高可用机制"></a>15、hdfs高可用机制</h3><p>hdfs高可用机制解决了namenode单点故障问题</p>
<p><img src="file:///Users/lankangning/Desktop/工作/image-20201021100427894.png?lastModify=1609225369" alt="image-20201021100427894"></p>
<h3 id="16、hdfs联邦机制"><a href="#16、hdfs联邦机制" class="headerlink" title="16、hdfs联邦机制"></a>16、hdfs联邦机制</h3><p>单namenode的架构在性能和拓展性上有潜在问题，当集群大的时候，namenode内存不够用，hdfs联邦机制解决了namenode拓展和性能的问题</p>
<p><img src="file:///Users/lankangning/Desktop/工作/image-20201021104659170.png?lastModify=1609225369" alt="image-20201021104659170"></p>
<h2 id="五、MapReduce"><a href="#五、MapReduce" class="headerlink" title="五、MapReduce"></a>五、MapReduce</h2><h3 id="1、MapReduce介绍"><a href="#1、MapReduce介绍" class="headerlink" title="1、MapReduce介绍"></a>1、MapReduce介绍</h3><p>分而治之</p>
<ul>
<li><p>map负责分：将复杂的任务分解为简单任务，小任务之间没有依赖关系</p>
</li>
<li><p>reduce负责合：对map阶段的结果进行全局汇总</p>
</li>
<li><p>MapReduce运行在yarn集群上</p>
</li>
</ul>
<h3 id="2、MapReduce编程流程"><a href="#2、MapReduce编程流程" class="headerlink" title="2、MapReduce编程流程"></a>2、MapReduce编程流程</h3><p>读文件——转成&lt;k1,v1&gt;——map——shuffle——&lt;k2,v2&gt;——reduce——&lt;k3,v3&gt;</p>
<h3 id="3、wordcount"><a href="#3、wordcount" class="headerlink" title="3、wordcount"></a>3、wordcount</h3><p>给定文本中统计输出每一个单词的出现次数</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 建立文本文件wordcount.txt</span></span><br><span class="line">cd /usr/lib</span><br><span class="line">vi wordcount.txt</span><br><span class="line"></span><br><span class="line">hello,world,hadoop</span><br><span class="line">hive,sqoop,flume,hello</span><br><span class="line">kitty,tom,jerry,world</span><br><span class="line">hadoop</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 上传到hdfs</span></span><br><span class="line">hdfs dfs -mkdir /wordcount</span><br><span class="line">hdfs dfs -put wordcount.txt /wordcount</span><br></pre></td></tr></table></figure>
<p>maven配置里添加</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">packaging</span>&gt;</span>jar<span class="tag">&lt;/<span class="name">packaging</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>wordcountmain类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定job任务（即wordcount整个流程）</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    1、读文件 inputformat 子类 TextInputFormat</span></span><br><span class="line"><span class="comment">    2、生成K1V1</span></span><br><span class="line"><span class="comment">    3、定义map逻辑，k1v1转为k2v2</span></span><br><span class="line"><span class="comment">    4、shuffle阶段</span></span><br><span class="line"><span class="comment">    5、得到新的k2v2</span></span><br><span class="line"><span class="comment">    6、定义reduce逻辑，k2v2转为k3v3</span></span><br><span class="line"><span class="comment">    7、k3v3写入新的文本文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="comment">//该方法用于指定一个job任务</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.创建一个job对象</span></span><br><span class="line">        <span class="comment">//super.getConf()调用父类方法，&quot;wordcount&quot;为job名称</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(),<span class="string">&quot;wordcount&quot;</span>);</span><br><span class="line">      	<span class="comment">// 如果打包运行出错，则需要加该配置</span></span><br><span class="line">      	<span class="comment">// job.setJarByClass(WordCountMain.class);</span></span><br><span class="line">        <span class="comment">//2.配置job任务对象</span></span><br><span class="line">        <span class="comment">//2.1.指定文件读取方式和路径</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;hdfs://node01:8020/wordcount&quot;</span>));</span><br><span class="line">        <span class="comment">//2.2.进行map阶段 设置mapper类，设置k2，v2的类型</span></span><br><span class="line">        job.setMapperClass(WordCountMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(LongWritable.class);</span><br><span class="line">        <span class="comment">//2.3.进行shuffle阶段，采用默认处理(暂未实现）</span></span><br><span class="line">        <span class="comment">//2.4.进行reduce阶段，设置reducer类，设置k2，v2的类型</span></span><br><span class="line">        job.setReducerClass(WordCountReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(LongWritable.class);</span><br><span class="line">        <span class="comment">//2.5.设置输出类型士</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;hdfs://node01:8020/wordcount_out&quot;</span>));</span><br><span class="line">        <span class="comment">//2.6.等待任务结束</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf,<span class="keyword">new</span> WordCountMain(),args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>wordcountMapper类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">四个泛型解释：</span></span><br><span class="line"><span class="comment">    keyin：k1的类型</span></span><br><span class="line"><span class="comment">    valuein：v1的类型</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    keyout：k2的类型</span></span><br><span class="line"><span class="comment">    valueout：v2的类型</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    // 不用java原来的基本类型</span></span><br><span class="line"><span class="comment">    &lt;Integer,String,String,Integer&gt;</span></span><br><span class="line"><span class="comment">    // 替代</span></span><br><span class="line"><span class="comment">    &lt;LongWritable, Text,Text,LongWritable&gt;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span> &lt;<span class="title">LongWritable</span>, <span class="title">Text</span>,<span class="title">Text</span>,<span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//重写map方法，即（k1,v1)——&gt;(k2,v2)</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        参数：</span></span><br><span class="line"><span class="comment">            LongWritable key：k1,行偏移量</span></span><br><span class="line"><span class="comment">            Text value:v1，每一行的文本数据</span></span><br><span class="line"><span class="comment">            Context context：上下文对象，给下一阶段的shuffler</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        如何将（k1,v1)——&gt;(k2,v2)</span></span><br><span class="line"><span class="comment">        （行偏移量，一行文本）——&gt;（单词，1）</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="comment">//将一行文本数据进行拆分成单词，遍历组装为K2，v2设置为1</span></span><br><span class="line">        String[] split = value.toString().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">        <span class="keyword">for</span>(String word:split)&#123;</span><br><span class="line">            context.write(<span class="keyword">new</span> Text(word),<span class="keyword">new</span> LongWritable(<span class="number">1</span>));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>wordcountReducer类</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">四个泛型解释：</span></span><br><span class="line"><span class="comment">    keyin：k2的类型</span></span><br><span class="line"><span class="comment">    valuein：v2的类型</span></span><br><span class="line"><span class="comment">    keyout：k3的类型</span></span><br><span class="line"><span class="comment">    valueout：v3的类型</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span> &lt;<span class="title">Text</span>, <span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">LongWritable</span>&gt;</span>&#123;</span><br><span class="line">    <span class="comment">//重写reduce方法</span></span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        将新的k2v2转为k3v3，将k3v3写入上下文</span></span><br><span class="line"><span class="comment">        Text key,新k2</span></span><br><span class="line"><span class="comment">        Iterable&lt;LongWritable&gt; values,新v2</span></span><br><span class="line"><span class="comment">        Context context，上下文</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(Text key, Iterable&lt;LongWritable&gt; values, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> count = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span>(LongWritable c:values)&#123;</span><br><span class="line">            count+=c.get();</span><br><span class="line">        &#125;</span><br><span class="line">        context.write(key,<span class="keyword">new</span> LongWritable(count));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="4、MapReduce运行模式"><a href="#4、MapReduce运行模式" class="headerlink" title="4、MapReduce运行模式"></a>4、MapReduce运行模式</h3><ol>
<li><p>集群运行模式</p>
<ol>
<li><p>将MapReduce程序提交给Yarn集群，分发到各个节点上并发执行</p>
</li>
<li><p>处理的数据和输出结果应该位于HDFS文件系统</p>
</li>
<li><p>提交集群的实现步骤：将程序打包成JAR包，并上传，然后在集群上用hadoop命令启动</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar hadoop-1.0-SNAPSHOT.jar com.lan.wordcount.WordCountMain</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br></pre></td><td class="code"><pre><span class="line">[root@node01 lib]# hadoop jar hadoop-1.0-SNAPSHOT.jar com.lan.wordcount.WordCountMain</span><br><span class="line">20/10/20 19:15:53 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id</span><br><span class="line">20/10/20 19:15:53 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=</span><br><span class="line">^Z</span><br><span class="line">[1]+  已停止               hadoop jar hadoop-1.0-SNAPSHOT.jar com.lan.wordcount.WordCountMain</span><br><span class="line">[root@node01 lib]# hadoop jar hadoop-1.0-SNAPSHOT.jar com.lan.wordcount.WordCountMain</span><br><span class="line">20/10/20 19:15:59 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id</span><br><span class="line">20/10/20 19:15:59 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=</span><br><span class="line">20/10/20 19:16:00 WARN mapreduce.JobResourceUploader: No job jar file set.  User classes may not be found. See Job or Job#setJar(String).</span><br><span class="line">20/10/20 19:16:00 INFO input.FileInputFormat: Total input files to process : 1</span><br><span class="line">20/10/20 19:16:00 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">20/10/20 19:16:00 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1402625500_0001</span><br><span class="line">20/10/20 19:16:01 INFO mapreduce.Job: The url to track the job: http://localhost:8080/</span><br><span class="line">20/10/20 19:16:01 INFO mapreduce.Job: Running job: job_local1402625500_0001</span><br><span class="line">20/10/20 19:16:01 INFO mapred.LocalJobRunner: OutputCommitter set in config null</span><br><span class="line">20/10/20 19:16:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1</span><br><span class="line">20/10/20 19:16:01 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false</span><br><span class="line">20/10/20 19:16:01 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter</span><br><span class="line">20/10/20 19:16:01 INFO mapred.LocalJobRunner: Waiting for map tasks</span><br><span class="line">20/10/20 19:16:01 INFO mapred.LocalJobRunner: Starting task: attempt_local1402625500_0001_m_000000_0</span><br><span class="line">20/10/20 19:16:01 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1</span><br><span class="line">20/10/20 19:16:01 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false</span><br><span class="line">20/10/20 19:16:01 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</span><br><span class="line">20/10/20 19:16:01 INFO mapred.MapTask: Processing split: hdfs://node01:8020/wordcount/wordcount.txt:0+71</span><br><span class="line">20/10/20 19:16:01 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)</span><br><span class="line">20/10/20 19:16:01 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100</span><br><span class="line">20/10/20 19:16:01 INFO mapred.MapTask: soft limit at 83886080</span><br><span class="line">20/10/20 19:16:01 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600</span><br><span class="line">20/10/20 19:16:01 INFO mapred.MapTask: kvstart = 26214396; length = 6553600</span><br><span class="line">20/10/20 19:16:01 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: </span><br><span class="line">20/10/20 19:16:02 INFO mapred.MapTask: Starting flush of map output</span><br><span class="line">20/10/20 19:16:02 INFO mapred.MapTask: Spilling map output</span><br><span class="line">20/10/20 19:16:02 INFO mapred.MapTask: bufstart = 0; bufend = 167; bufvoid = 104857600</span><br><span class="line">20/10/20 19:16:02 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214352(104857408); length = 45/6553600</span><br><span class="line">20/10/20 19:16:02 INFO mapred.MapTask: Finished spill 0</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Task: Task:attempt_local1402625500_0001_m_000000_0 is done. And is in the process of committing</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: map</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Task: Task &#x27;attempt_local1402625500_0001_m_000000_0&#x27; done.</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Task: Final Counters for attempt_local1402625500_0001_m_000000_0: Counters: 22</span><br><span class="line">        File System Counters</span><br><span class="line">                FILE: Number of bytes read=172</span><br><span class="line">                FILE: Number of bytes written=522975</span><br><span class="line">                FILE: Number of read operations=0</span><br><span class="line">                FILE: Number of large read operations=0</span><br><span class="line">                FILE: Number of write operations=0</span><br><span class="line">                HDFS: Number of bytes read=71</span><br><span class="line">                HDFS: Number of bytes written=0</span><br><span class="line">                HDFS: Number of read operations=5</span><br><span class="line">                HDFS: Number of large read operations=0</span><br><span class="line">                HDFS: Number of write operations=1</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">                Map input records=4</span><br><span class="line">                Map output records=12</span><br><span class="line">                Map output bytes=167</span><br><span class="line">                Map output materialized bytes=197</span><br><span class="line">                Input split bytes=107</span><br><span class="line">                Combine input records=0</span><br><span class="line">                Spilled Records=12</span><br><span class="line">                Failed Shuffles=0</span><br><span class="line">                Merged Map outputs=0</span><br><span class="line">                GC time elapsed (ms)=46</span><br><span class="line">                Total committed heap usage (bytes)=149622784</span><br><span class="line">        File Input Format Counters </span><br><span class="line">                Bytes Read=71</span><br><span class="line">20/10/20 19:16:02 INFO mapreduce.Job: Job job_local1402625500_0001 running in uber mode : false</span><br><span class="line">20/10/20 19:16:02 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: Finishing task: attempt_local1402625500_0001_m_000000_0</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: map task executor complete.</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: Waiting for reduce tasks</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: Starting task: attempt_local1402625500_0001_r_000000_0</span><br><span class="line">20/10/20 19:16:02 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 1</span><br><span class="line">20/10/20 19:16:02 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]</span><br><span class="line">20/10/20 19:16:02 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@411f56d7</span><br><span class="line">20/10/20 19:16:02 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10</span><br><span class="line">20/10/20 19:16:02 INFO reduce.EventFetcher: attempt_local1402625500_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events</span><br><span class="line">20/10/20 19:16:02 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1402625500_0001_m_000000_0 decomp: 193 len: 197 to MEMORY</span><br><span class="line">20/10/20 19:16:02 INFO reduce.InMemoryMapOutput: Read 193 bytes from map-output for attempt_local1402625500_0001_m_000000_0</span><br><span class="line">20/10/20 19:16:02 INFO reduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 193, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;193</span><br><span class="line">20/10/20 19:16:02 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning</span><br><span class="line">20/10/20 19:16:02 WARN io.ReadaheadPool: Failed readahead on ifile</span><br><span class="line">EBADF: Bad file descriptor</span><br><span class="line">        at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)</span><br><span class="line">        at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)</span><br><span class="line">        at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)</span><br><span class="line">        at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:208)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)</span><br><span class="line">        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)</span><br><span class="line">        at java.lang.Thread.run(Thread.java:748)</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: 1 / 1 copied.</span><br><span class="line">20/10/20 19:16:02 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Merger: Merging 1 sorted segments</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 185 bytes</span><br><span class="line">20/10/20 19:16:02 INFO reduce.MergeManagerImpl: Merged 1 segments, 193 bytes to disk to satisfy reduce memory limit</span><br><span class="line">20/10/20 19:16:02 INFO reduce.MergeManagerImpl: Merging 1 files, 197 bytes from disk</span><br><span class="line">20/10/20 19:16:02 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Merger: Merging 1 sorted segments</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 185 bytes</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: 1 / 1 copied.</span><br><span class="line">20/10/20 19:16:02 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Task: Task:attempt_local1402625500_0001_r_000000_0 is done. And is in the process of committing</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: 1 / 1 copied.</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Task: Task attempt_local1402625500_0001_r_000000_0 is allowed to commit now</span><br><span class="line">20/10/20 19:16:02 INFO output.FileOutputCommitter: Saved output of task &#x27;attempt_local1402625500_0001_r_000000_0&#x27; to hdfs://node01:8020/wordcount_out/_temporary/0/task_local1402625500_0001_r_000000</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: reduce &gt; reduce</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Task: Task &#x27;attempt_local1402625500_0001_r_000000_0&#x27; done.</span><br><span class="line">20/10/20 19:16:02 INFO mapred.Task: Final Counters for attempt_local1402625500_0001_r_000000_0: Counters: 29</span><br><span class="line">        File System Counters</span><br><span class="line">                FILE: Number of bytes read=598</span><br><span class="line">                FILE: Number of bytes written=523172</span><br><span class="line">                FILE: Number of read operations=0</span><br><span class="line">                FILE: Number of large read operations=0</span><br><span class="line">                FILE: Number of write operations=0</span><br><span class="line">                HDFS: Number of bytes read=71</span><br><span class="line">                HDFS: Number of bytes written=70</span><br><span class="line">                HDFS: Number of read operations=8</span><br><span class="line">                HDFS: Number of large read operations=0</span><br><span class="line">                HDFS: Number of write operations=3</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">                Combine input records=0</span><br><span class="line">                Combine output records=0</span><br><span class="line">                Reduce input groups=9</span><br><span class="line">                Reduce shuffle bytes=197</span><br><span class="line">                Reduce input records=12</span><br><span class="line">                Reduce output records=9</span><br><span class="line">                Spilled Records=12</span><br><span class="line">                Shuffled Maps =1</span><br><span class="line">                Failed Shuffles=0</span><br><span class="line">                Merged Map outputs=1</span><br><span class="line">                GC time elapsed (ms)=2</span><br><span class="line">                Total committed heap usage (bytes)=149622784</span><br><span class="line">        Shuffle Errors</span><br><span class="line">                BAD_ID=0</span><br><span class="line">                CONNECTION=0</span><br><span class="line">                IO_ERROR=0</span><br><span class="line">                WRONG_LENGTH=0</span><br><span class="line">                WRONG_MAP=0</span><br><span class="line">                WRONG_REDUCE=0</span><br><span class="line">        File Output Format Counters </span><br><span class="line">                Bytes Written=70</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: Finishing task: attempt_local1402625500_0001_r_000000_0</span><br><span class="line">20/10/20 19:16:02 INFO mapred.LocalJobRunner: reduce task executor complete.</span><br><span class="line">20/10/20 19:16:03 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">20/10/20 19:16:03 INFO mapreduce.Job: Job job_local1402625500_0001 completed successfully</span><br><span class="line">20/10/20 19:16:03 INFO mapreduce.Job: Counters: 35</span><br><span class="line">        File System Counters</span><br><span class="line">                FILE: Number of bytes read=770</span><br><span class="line">                FILE: Number of bytes written=1046147</span><br><span class="line">                FILE: Number of read operations=0</span><br><span class="line">                FILE: Number of large read operations=0</span><br><span class="line">                FILE: Number of write operations=0</span><br><span class="line">                HDFS: Number of bytes read=142</span><br><span class="line">                HDFS: Number of bytes written=70</span><br><span class="line">                HDFS: Number of read operations=13</span><br><span class="line">                HDFS: Number of large read operations=0</span><br><span class="line">                HDFS: Number of write operations=4</span><br><span class="line">        Map-Reduce Framework</span><br><span class="line">                Map input records=4</span><br><span class="line">                Map output records=12</span><br><span class="line">                Map output bytes=167</span><br><span class="line">                Map output materialized bytes=197</span><br><span class="line">                Input split bytes=107</span><br><span class="line">                Combine input records=0</span><br><span class="line">                Combine output records=0</span><br><span class="line">                Reduce input groups=9</span><br><span class="line">                Reduce shuffle bytes=197</span><br><span class="line">                Reduce input records=12</span><br><span class="line">                Reduce output records=9</span><br><span class="line">                Spilled Records=24</span><br><span class="line">                Shuffled Maps =1</span><br><span class="line">                Failed Shuffles=0</span><br><span class="line">                Merged Map outputs=1</span><br><span class="line">                GC time elapsed (ms)=48</span><br><span class="line">                Total committed heap usage (bytes)=299245568</span><br><span class="line">        Shuffle Errors</span><br><span class="line">                BAD_ID=0</span><br><span class="line">                CONNECTION=0</span><br><span class="line">                IO_ERROR=0</span><br><span class="line">                WRONG_LENGTH=0</span><br><span class="line">                WRONG_MAP=0</span><br><span class="line">                WRONG_REDUCE=0</span><br><span class="line">        File Input Format Counters </span><br><span class="line">                Bytes Read=71</span><br><span class="line">        File Output Format Counters </span><br><span class="line">                Bytes Written=70</span><br><span class="line">[root@node01 lib]# hdfs dfs -ls /wordcount_out</span><br><span class="line">Found 2 items</span><br><span class="line">-rw-r--r--   3 root supergroup          0 2020-10-20 19:16 /wordcount_out/_SUCCESS</span><br><span class="line">-rw-r--r--   3 root supergroup         70 2020-10-20 19:16 /wordcount_out/part-r-00000</span><br><span class="line">[root@node01 lib]# hdfs dfs -cat /wordcount_out/part-r-00000</span><br><span class="line">flume   1</span><br><span class="line">hadoop  2</span><br><span class="line">hello   2</span><br><span class="line">hive    1</span><br><span class="line">jerry   1</span><br><span class="line">kitty   1</span><br><span class="line">sqoop   1</span><br><span class="line">tom     1</span><br><span class="line">world   2</span><br><span class="line"></span><br></pre></td></tr></table></figure>


</li>
</ol>
</li>
</ol>
<ol start="2">
<li><p>本地运行</p>
<p>java程序直接运行，outputformat改为本地文件系统地址，一般测试用.</p>
<p><strong>不管是本地运行还是集群运行，输出目录都不能事先存在</strong></p>
</li>
</ol>
<h3 id="5、MapReduce分区（shuffle第一阶段）"><a href="#5、MapReduce分区（shuffle第一阶段）" class="headerlink" title="5、MapReduce分区（shuffle第一阶段）"></a>5、MapReduce分区（shuffle第一阶段）</h3><p><strong>概述：</strong></p>
<p>指定分区，将同一个分区的数据发送到同一个reduce中进行处理，（给数据打上标记）</p>
<p>例如:为了数据的统计,可以把一批类似的数据发送到同一个Reduce当中,在同一个Reduce当中统计相同类型的数据,就可以实现类似的数据分区和统计等</p>
<p>其实就是相同类型的数据,有共性的数据，送到一起去处理</p>
<p><img src="/Users/lankangning/Desktop/%E5%B7%A5%E4%BD%9C/image-20201021200158943.png" alt="image-20201021200158943"></p>
<p>产生两个结果</p>
<p><strong>分区步骤</strong></p>
<p>1.写mapper</p>
<p>2.写mypartitioner</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.wordcount_partition.wordcount;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Partitioner;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">k2,v2</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyPartitioner</span> <span class="keyword">extends</span> <span class="title">Partitioner</span>&lt;<span class="title">Text</span>, <span class="title">LongWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="comment">/*</span></span><br><span class="line"><span class="comment">        1.定义分区规则：字符串长度&lt;=4的</span></span><br><span class="line"><span class="comment">        2.返回分区编号</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getPartition</span><span class="params">(Text text, LongWritable longWritable, <span class="keyword">int</span> i)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(text.toString().length()&lt;=<span class="number">4</span>)&#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>3.写reducer</p>
<p>4.写jobmain</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">job.setPartitionerClass(MyPartitioner.class);</span><br><span class="line">job.setNumReduceTasks(<span class="number">2</span>);</span><br></pre></td></tr></table></figure>
<h3 id="6、MapReduce计数器"><a href="#6、MapReduce计数器" class="headerlink" title="6、MapReduce计数器"></a>6、MapReduce计数器</h3><ol>
<li><p>内置计数器</p>
</li>
<li><p>自定义计数器</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//    枚举计数器</span></span><br><span class="line"><span class="comment">//    public static enum Counter&#123;</span></span><br><span class="line"><span class="comment">//        MY_COUNTER0,MY_COUNTER1</span></span><br><span class="line"><span class="comment">//    &#125;</span></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line"><span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span> <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line">    <span class="comment">//将一行文本数据进行拆分成单词，遍历组装为K2，v2设置为1</span></span><br><span class="line">    String[] split = value.toString().split(<span class="string">&quot;,&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span>(String word:split)&#123;</span><br><span class="line">      <span class="comment">//单个计数器</span></span><br><span class="line">      <span class="comment">// parm0:计数器的类型，parm1：计数器的名字</span></span><br><span class="line">      <span class="comment">// Counter counter = context.getCounter(&quot;MR_COUNT&quot;,&quot;MyRecordCounter&quot;);</span></span><br><span class="line">      <span class="comment">// counter.increment(1L);</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">// 枚举计数器</span></span><br><span class="line">      <span class="comment">//context.getCounter(Counter.MY_COUNTER0).increment(1L);</span></span><br><span class="line"></span><br><span class="line">      context.write(<span class="keyword">new</span> Text(word),<span class="keyword">new</span> LongWritable(<span class="number">1</span>));</span><br><span class="line">  &#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="7-、MapReduce排序和序列化"><a href="#7-、MapReduce排序和序列化" class="headerlink" title="7 、MapReduce排序和序列化"></a>7 、MapReduce排序和序列化</h3></li>
</ol>
<ul>
<li>序列化：指把结构化对象转化为字节流</li>
<li>逆序列化：指把字节流转化为结构化对象</li>
<li>java的序列化是一个重量级序列化框架，不便于在网络中高效传输</li>
<li>writable是hadoop的序列化格式</li>
<li>writablecomparable子接口可以用于排序</li>
<li><strong>序列化的顺序和反序列化顺序必须一致</strong></li>
</ul>
<p>demo：</p>
<p>jobmain.class</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.sortword;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">a 1</span></span><br><span class="line"><span class="comment">a 9</span></span><br><span class="line"><span class="comment">b 3</span></span><br><span class="line"><span class="comment">a 7</span></span><br><span class="line"><span class="comment">b 8</span></span><br><span class="line"><span class="comment">b 10</span></span><br><span class="line"><span class="comment">a 5</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line">排序为</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">first=a, second=1</span></span><br><span class="line"><span class="comment">first=a, second=5</span></span><br><span class="line"><span class="comment">first=a, second=7</span></span><br><span class="line"><span class="comment">first=a, second=9</span></span><br><span class="line"><span class="comment">first=b, second=3</span></span><br><span class="line"><span class="comment">first=b, second=8</span></span><br><span class="line"><span class="comment">first=b, second=10</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JobMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(),<span class="string">&quot;sort&quot;</span>);</span><br><span class="line"></span><br><span class="line">        job.setMapperClass(SortWordMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(SortBean.class);</span><br><span class="line">        job.setMapOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;file:///Users/lankangning/desktop/sortword_in_local&quot;</span>));</span><br><span class="line"></span><br><span class="line">        job.setReducerClass(SortWordReducer.class);</span><br><span class="line">        job.setOutputKeyClass(SortBean.class);</span><br><span class="line">        job.setOutputValueClass(NullWritable.class);</span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;file:///Users/lankangning/desktop/sortword_out_local&quot;</span>));</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> b = job.waitForCompletion(<span class="keyword">true</span>)? <span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> b;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf,<span class="keyword">new</span> JobMain(),args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>sortbean.class</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.sortword;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.DataInput;</span><br><span class="line"><span class="keyword">import</span> java.io.DataOutput;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.WritableComparable;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortBean</span> <span class="keyword">implements</span> <span class="title">WritableComparable</span>&lt;<span class="title">SortBean</span>&gt; </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String word;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> num;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SortBean</span><span class="params">()</span></span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">SortBean</span><span class="params">(String word,<span class="keyword">int</span> num)</span></span>&#123;</span><br><span class="line">        <span class="keyword">super</span>();</span><br><span class="line">        <span class="keyword">this</span>.word = word;</span><br><span class="line">        <span class="keyword">this</span>.num = num;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">getWord</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> word;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setWord</span><span class="params">(String word)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.word = word;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getNum</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> num; &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setNum</span><span class="params">(<span class="keyword">int</span> num)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.num = num;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//序列化顺序和反序列化顺序必须一致</span></span><br><span class="line">    <span class="comment">//实现反序列</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">readFields</span><span class="params">(DataInput dataInput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.word = dataInput.readUTF();</span><br><span class="line">        <span class="keyword">this</span>.num = dataInput.readInt();</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//实现序列化</span></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">write</span><span class="params">(DataOutput dataOutput)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">        dataOutput.writeUTF(<span class="keyword">this</span>.word);</span><br><span class="line">        dataOutput.writeInt(<span class="keyword">this</span>.num);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compareTo</span><span class="params">(SortBean o)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> result = <span class="keyword">this</span>.word.compareTo(o.word);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (result == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">this</span>.num-o.num;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">toString</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">&quot;first=&quot;</span> + word + <span class="string">&quot;, second=&quot;</span> + num;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Mapper.class</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.sortword;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Mapper;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortWordMapper</span> <span class="keyword">extends</span> <span class="title">Mapper</span>&lt;<span class="title">LongWritable</span>, <span class="title">Text</span>, <span class="title">SortBean</span>, <span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">map</span><span class="params">(LongWritable key, Text value, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        String[] split = value.toString().split(<span class="string">&quot; &quot;</span>);</span><br><span class="line"></span><br><span class="line">        SortBean sortBean = <span class="keyword">new</span> SortBean();</span><br><span class="line">        sortBean.setWord(split[<span class="number">0</span>]);</span><br><span class="line">        sortBean.setNum(Integer.parseInt(split[<span class="number">1</span>]));</span><br><span class="line"></span><br><span class="line">        <span class="comment">//2:将K2和V2写入上下文中</span></span><br><span class="line">        context.write(sortBean, NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>reducer</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.sortword;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.NullWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Reducer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SortWordReducer</span> <span class="keyword">extends</span> <span class="title">Reducer</span>&lt;<span class="title">SortBean</span>, <span class="title">NullWritable</span>, <span class="title">SortBean</span>,<span class="title">NullWritable</span>&gt; </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">protected</span> <span class="keyword">void</span> <span class="title">reduce</span><span class="params">(SortBean key, Iterable&lt;NullWritable&gt; values, Context context)</span></span></span><br><span class="line"><span class="function">            <span class="keyword">throws</span> IOException, InterruptedException </span>&#123;</span><br><span class="line"></span><br><span class="line">        context.write(key,NullWritable.get());</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="8、MapReduce规约combiner"><a href="#8、MapReduce规约combiner" class="headerlink" title="8、MapReduce规约combiner"></a>8、MapReduce规约combiner</h3><p><img src="/Users/lankangning/Desktop/%E5%B7%A5%E4%BD%9C/image-20201024143514209.png" alt="image-20201024143514209"></p>
<p>简单来说combiner类似于reducer，combiner的作用就是对map端的输出先做一次合并，以减少map和reduce节点之间的数据传输，提高IO性能，是MapReduce的优化手段之一。把reduce要做的事提前先做一下</p>
<ul>
<li><p>combiner是mapper和reducer之外的一种组件</p>
</li>
<li><p>combiner组件的父类是reducer</p>
</li>
<li><p>combiner和reducer的区别</p>
<ul>
<li>combiner是在每一个maptask所在的节点运行</li>
<li>reducer是接受全局所有的mapper的输出结果</li>
</ul>
</li>
<li><p>combiner的意义就是对每一个maptask的输出进行局部汇总，减少网络传输量</p>
</li>
</ul>
<figure class="highlight re"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># wordcount任务</span><br><span class="line"># combiner之前</span><br><span class="line">Reduce input records=<span class="number">9</span></span><br><span class="line">Reduce output records=<span class="number">6</span></span><br><span class="line"></span><br><span class="line"># combiner之后</span><br><span class="line">Reduce input records=<span class="number">6</span></span><br><span class="line">Reduce output records=<span class="number">6</span></span><br></pre></td></tr></table></figure>
<p>mycombiner.class</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lan.wordcount_combiner;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configured;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.LongWritable;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.io.Text;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.Job;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.input.TextInputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.Tool;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.util.ToolRunner;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 指定job任务（即wordcount整个流程）</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    1、读文件 inputformat 子类 TextInputFormat</span></span><br><span class="line"><span class="comment">    2、生成K1V1</span></span><br><span class="line"><span class="comment">    3、定义map逻辑，k1v1转为k2v2</span></span><br><span class="line"><span class="comment">    4、shuffle阶段</span></span><br><span class="line"><span class="comment">    5、得到新的k2v2</span></span><br><span class="line"><span class="comment">    6、定义reduce逻辑，k2v2转为k3v3</span></span><br><span class="line"><span class="comment">    7、k3v3写入新的文本文件</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountCombinerMain</span> <span class="keyword">extends</span> <span class="title">Configured</span> <span class="keyword">implements</span> <span class="title">Tool</span> </span>&#123;</span><br><span class="line">    <span class="comment">//该方法用于指定一个job任务</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">run</span><span class="params">(String[] strings)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        <span class="comment">//1.创建一个job对象</span></span><br><span class="line">        <span class="comment">//super.getConf()调用父类方法，&quot;wordcount&quot;为job名称</span></span><br><span class="line">        Job job = Job.getInstance(<span class="keyword">super</span>.getConf(),<span class="string">&quot;wordcount&quot;</span>);</span><br><span class="line">        <span class="comment">// 如果打包运行出错，则需要加该配置</span></span><br><span class="line">        <span class="comment">// job.setJarByClass(WordCountMain.class);</span></span><br><span class="line">        <span class="comment">//2.配置job任务对象</span></span><br><span class="line">        <span class="comment">//2.1.指定文件读取方式和路径</span></span><br><span class="line">        job.setInputFormatClass(TextInputFormat.class);</span><br><span class="line">        TextInputFormat.addInputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;file:///Users/lankangning/desktop/wordcount_in_local&quot;</span>));</span><br><span class="line">        <span class="comment">//2.2.进行map阶段 设置mapper类，设置k2，v2的类型</span></span><br><span class="line">        job.setMapperClass(WordCountCombinerMapper.class);</span><br><span class="line">        job.setMapOutputKeyClass(Text.class);</span><br><span class="line">        job.setMapOutputValueClass(LongWritable.class);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        <span class="comment">//设置combiner类型</span></span><br><span class="line">        job.setCombinerClass(MyCombiner.class);</span><br><span class="line">        <span class="comment">//2.4.进行reduce阶段，设置reducer类，设置k2，v2的类型</span></span><br><span class="line">        job.setReducerClass(WordCountCombinerReducer.class);</span><br><span class="line">        job.setOutputKeyClass(Text.class);</span><br><span class="line">        job.setOutputValueClass(LongWritable.class);</span><br><span class="line">        <span class="comment">//2.5.设置输出类型士</span></span><br><span class="line">        job.setOutputFormatClass(TextOutputFormat.class);</span><br><span class="line">        TextOutputFormat.setOutputPath(job,<span class="keyword">new</span> Path(<span class="string">&quot;file:///Users/lankangning/desktop/wordcount_out_local&quot;</span>));</span><br><span class="line">        <span class="comment">//2.6.等待任务结束</span></span><br><span class="line">        <span class="keyword">boolean</span> b = job.waitForCompletion(<span class="keyword">true</span>);</span><br><span class="line">        <span class="keyword">return</span> b ? <span class="number">0</span>:<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line">        Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">        <span class="keyword">int</span> run = ToolRunner.run(conf,<span class="keyword">new</span> WordCountCombinerMain(),args);</span><br><span class="line">        System.exit(run);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Jobmain.class</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//规约</span></span><br><span class="line"><span class="comment">//设置combiner类型</span></span><br><span class="line">job.setCombinerClass(MyCombiner.class);</span><br></pre></td></tr></table></figure>
<p>​                                                                                2020年10月26日22:09:43</p>
<hr>
<h3 id="9、MapReduce工作机制"><a href="#9、MapReduce工作机制" class="headerlink" title="9、MapReduce工作机制"></a>9、MapReduce工作机制</h3><p>maptask：</p>
<p>文件——HDFS——maptask——partition——环形缓冲区——spill——排序——combiner——小文件——merge——排序——临时文件</p>
<p>网络连接</p>
<p>reducetask：</p>
<p>（拉取数据到reduce缓冲区——spill——小文件——merge——排序——<strong>分组</strong>（相同key的数据，value放在同一个集合）——中间文件）——reducetask——输出——part-r-0000</p>
<h2 id="六、yarn"><a href="#六、yarn" class="headerlink" title="六、yarn"></a>六、yarn</h2><h3 id="1、yarn介绍"><a href="#1、yarn介绍" class="headerlink" title="1、yarn介绍"></a>1、yarn介绍</h3><p>yarn是hadoop集群中的资源管理系统模块，主要用于<strong>管理集群资源</strong>（主要是硬件），调度运行再yarn上面的各种任务，<strong>调度任务</strong>。</p>
<p>其调度分为两个层级来说：</p>
<ul>
<li><p>一级调度管理：</p>
<p>​    计算资源管理(CPU,内存，网络IO，磁盘)</p>
</li>
<li><p>二级调度管理：</p>
<p>​    任务内部的计算模型管理  (AppMaster的任务精细化管理)</p>
</li>
</ul>
<h3 id="2、Yarn的主要组件介绍与作用"><a href="#2、Yarn的主要组件介绍与作用" class="headerlink" title="2、Yarn的主要组件介绍与作用"></a>2、<strong>Yarn的</strong>主要组件介绍与作用</h3><p>  <code>YARN总体上是Master/Slave结构</code>，主要由ResourceManager、NodeManager、 ApplicationMaster和Container等几个组件构成。</p>
<ul>
<li><strong>ResourceManager(RM)</strong><br>负责处理客户端请求,对各NM上的资源进行统一管理和调度。给ApplicationMaster分配空闲的Container 运行并监控其运行状态。主要由两个组件构成：调度器和应用程序管理器：<ol>
<li><strong>调度器(Scheduler)**：调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的应用程序。调度器</strong>仅<strong>根据各个应用程序的资源需求进行资源分配，而资源分配单位是Container。Shceduler</strong>不负责监控或者跟踪应用程序的状态**。总之，调度器根据应用程序的资源要求，以及集群机器的资源情况，为应用程序分配封装在Container中的资源。 </li>
<li>**应用程序管理器(Applications Manager)**：应用程序管理器负责管理整个系统中所有应用程序，包括应用程序提交、与调度器协商资源以启动ApplicationMaster 、监控ApplicationMaster运行状态并在失败时重新启动等，跟踪分给的Container的进度、状态也是其职责。</li>
</ol>
</li>
<li><strong>NodeManager (NM)</strong><br>NodeManager 是每个节点上的资源和任务管理器。它会定时地向ResourceManager<strong>汇报</strong>本节点上的资源使用情况和各个Container的运行状态；同时会接收并<strong>处理</strong>来自ApplicationMaster 的Container 启动/停止等<strong>请求</strong>。</li>
<li><strong>ApplicationMaster (AM)**：负责整个任务的调度管理<br>用户提交的应用程序均包含一个</strong>ApplicationMaster **，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。</li>
<li><strong>Container</strong>：<br>Container是YARN中的资源抽象，它<strong>封装了某个节点上的多维度资源</strong>，如内存、CPU、磁盘、网络等，当ApplicationMaster向ResourceManager申请资源时，ResourceManager为ApplicationMaster 返回的资源便是用Container 表示的。</li>
</ul>
<h3 id="3、Yarn的工作流程"><a href="#3、Yarn的工作流程" class="headerlink" title="3、Yarn的工作流程"></a>3、Yarn的工作流程</h3><h3 id="4、yarn的调度器"><a href="#4、yarn的调度器" class="headerlink" title="4、yarn的调度器"></a>4、yarn的调度器</h3><p>yarn我们都知道主要是用于做资源调度，任务分配等功能的，那么在hadoop当中，究竟使用什么算法来进行任务调度就需要我们关注了，hadoop支持好几种任务的调度方式，不同的场景需要使用不同的任务调度器.</p>
<h5 id="第一种调度器：FIFO-Scheduler（队列调度）"><a href="#第一种调度器：FIFO-Scheduler（队列调度）" class="headerlink" title="第一种调度器：FIFO Scheduler（队列调度）"></a><code>第一种调度器：FIFO Scheduler（队列调度）</code></h5><p>把任务按提交的顺序排成一个队列，这是一个先进先出队列，<strong>在进行资源分配的时候，先给队列中最头上的任务进行分配资源</strong>，待最头上任务需求满足后再给下一个分配，以此类推。只根据资源分配来调度，资源不足的时候后边的job只能等待。</p>
<p>FIFO Scheduler是最简单也是最容易理解的调度器，也不需要任何配置，但它并不适用于共享集群。大的任务可能会占用所有集群资源，这就导致其它任务被阻塞。</p>
<h5 id="第二种调度器：Capacity-Scheduler（容量调度器，apache版本默认使用的调度器）"><a href="#第二种调度器：Capacity-Scheduler（容量调度器，apache版本默认使用的调度器）" class="headerlink" title="第二种调度器：Capacity Scheduler（容量调度器，apache版本默认使用的调度器）"></a><code>第二种调度器：Capacity Scheduler（容量调度器，apache版本默认使用的调度器）</code></h5><p>Capacity 调度器允许多个组织共享整个集群，每个组织可以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，资源的调度是采用的是先进先出(FIFO)策略。</p>
<p><img src="/Users/lankangning/Desktop/%E5%B7%A5%E4%BD%9C/image-20201027104043022.png" alt="image-20201027104043022"></p>
<h5 id="第三种调度器：Fair-Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）"><a href="#第三种调度器：Fair-Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）" class="headerlink" title="第三种调度器：Fair Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）"></a><code>第三种调度器：Fair Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）</code></h5><p>Fair调度器的设计目标是为所有的应用分配公平的资源（对公平的定义可以通过参数来设置）。公平调度在也可以在多个队列间工作。举个例子，假设有两个用户A和B，他们分别拥有一个队列。当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之一的集群资源，而A的job仍然用于集群一半的资源，<strong>结果就是资源最终在两个用户之间平等的共享</strong></p>
<p>使用哪种调度器取决于yarn-site.xml当中的</p>
<p><code>yarn.resourcemanager.scheduler.class</code>  这个属性的配置</p>
<h3 id="5、关于yarn常用参数设置"><a href="#5、关于yarn常用参数设置" class="headerlink" title="5、关于yarn常用参数设置"></a>5、<strong>关于</strong>yarn常用参数设置</h3><h4 id="设置container分配最小内存"><a href="#设置container分配最小内存" class="headerlink" title="设置container分配最小内存"></a>设置container分配<strong>最小内存</strong></h4><p> yarn.scheduler.minimum-allocation-mb      1024   给应用程序container分配的最小内存</p>
<h4 id="设置container分配最大内存"><a href="#设置container分配最大内存" class="headerlink" title="设置container分配最大内存"></a>设置container分配<strong>最大内存</strong></h4><p> yarn.scheduler.maximum-allocation-mb      8192    给应用程序container分配的最大内存</p>
<h4 id="设置每个container的最小虚拟内核个数"><a href="#设置每个container的最小虚拟内核个数" class="headerlink" title="设置每个container的最小虚拟内核个数"></a>设置每个<strong>container的</strong>最小<strong>虚拟内核个数</strong></h4><p>yarn.scheduler.minimum-allocation-vcores      1      每个container默认给分配的最小的虚拟内核个数</p>
<h4 id="设置每个container的最大虚拟内核个数"><a href="#设置每个container的最大虚拟内核个数" class="headerlink" title="设置每个container的最大虚拟内核个数"></a>设置每个container的最大虚拟内核个数</h4><p>  yarn.scheduler.maximum-allocation-vcores      32  每个container可以分配的最大的虚拟内核的个数</p>
<h4 id="设置NodeManager可以分配的内存大小"><a href="#设置NodeManager可以分配的内存大小" class="headerlink" title="设置NodeManager可以分配的内存大小"></a>设置NodeManager可以分配的内存大小</h4><p>yarn.nodemanager.resource.memory-mb   8192  nodemanager可以分配的最大内存大小，默认8192Mb</p>
<p>####定义每台机器的内存使用大小</p>
<p> yarn.nodemanager.resource.memory-mb  8192</p>
<h4 id="定义交换区空间可以使用的大小"><a href="#定义交换区空间可以使用的大小" class="headerlink" title="定义交换区空间可以使用的大小"></a>定义交换区空间可以使用的大小</h4><p> 交换区空间就是讲一块硬盘拿出来做内存使用,这里指定的是nodemanager的2.1倍</p>
<p>yarn.nodemanager.vmem-pmem-ratio   2.1  </p>
<p>​                                                                                2020年10月27日21:18:27</p>
<hr>
<h2 id="七、Hive"><a href="#七、Hive" class="headerlink" title="七、Hive"></a>七、Hive</h2><h3 id="1、数据仓库"><a href="#1、数据仓库" class="headerlink" title="1、数据仓库"></a>1、数据仓库</h3><ol>
<li><p>基本概念：DataWareHouse，目的是构建面向分析的集成化数据环境，为企业提供决策支持。面向分析的存储系统</p>
</li>
<li><p>主要特征：<strong>面向主题</strong>，<strong>集成性</strong>（数据在进入数据仓库之前必然要经过统一与整合，这是最关键最复杂的一步ETL），<strong>非易失性</strong>（数仓中的数据一般仅执行查询操作，很少有删除和更新，但是需要定期加载和刷新数据），<strong>时变性</strong>（数据仓库包含各种粒度的历史数据，可能与时间有关，数据需要定时更新，以适应决策需要）</p>
</li>
<li><p>数据库和数据仓库的区别</p>
<p>数据库与数据仓库的区别实际讲的是 <code>OLTP</code> 与 <code>OLAP</code> 的区别。</p>
<p>操作型处理，叫联机事务处理 OLTP（On-Line Transaction Processing，），也可以称面向交易的处理系统，它是针对具体业务在数据库联机的日常操作，通常对少数记录进行查询、修改。用户较为关心操作的响应时间、数据的安全性、完整性和并发支持的用户数等问题。传统的数据库系统作为数据管理的主要手段，主要用于操作型处理。</p>
<p>分析型处理，叫联机分析处理 OLAP（On-Line Analytical Processing）一般针对某些主题的历史数据进行分析，支持 管理决策。</p>
<table>
<thead>
<tr>
<th>数据库</th>
<th>数据仓库</th>
</tr>
</thead>
<tbody><tr>
<td>面向事务</td>
<td>面向主题</td>
</tr>
<tr>
<td>一般存储业务数据</td>
<td>一般存储历史数据</td>
</tr>
<tr>
<td>尽量避免冗余，一般针对某一业务应用设计</td>
<td>有意引入冗余，依照分析需求设计</td>
</tr>
<tr>
<td>为了捕获数据设计</td>
<td>为分析数据设计</td>
</tr>
</tbody></table>
<p>首先要明白，数据仓库的出现，并不是要取代数据库。数仓是在数据库大量存在的情况下，为了进一步挖掘数据资源，为了决策需求而产生的。</p>
</li>
<li><p>数据仓库的分层架构</p>
<p><img src="/Users/lankangning/Desktop/%E5%B7%A5%E4%BD%9C/image-20201028103409902.png" alt="image-20201028103409902"></p>
<ul>
<li><code>源数据层（ODS）</code>：此层数据无任何更改，直接沿用外围系统数据结构和数据，<strong>不对外开放</strong>；<strong>为临时存储层</strong>，是接口数据的临时存储区域，为后一步的数据处理（ETL）做准备。</li>
<li><code>数据仓库层（DW）</code>：也称为细节层，DW层的数据应该是一致的、准确的、干净的数据，即对源系统数据进行了清洗（去除了杂质）后的数据。</li>
<li><code>数据应用层（DA或APP）</code>：前端应用直接读取的数据源；根据报表、专题分析需求而计算生成的数据。</li>
</ul>
<p>数据仓库从各数据源获取数据及在数据仓库内的数据转换和流动都可以认为是ETL（抽取Extra, 转化Transfer, 装载Load）的过程，ETL是数据仓库的流水线，也可以认为是数据仓库的血液，它维系着数据仓库中数据的新陈代谢，而数据仓库日常的管理和维护工作的大部分精力就是保持ETL的正常和稳定。</p>
</li>
<li><p>数仓的元数据管理</p>
<p>元数据（Meta Date），主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及ETL的任务运行状态。一般会通过元数据资料库（Metadata Repository）来统一地存储和管理元数据，其主要目的是使数据仓库的设计、部署、操作和管理能达成协同和一致。</p>
<p>元数据是数据仓库管理系统的重要组成部分，元数据管理是企业级数据仓库中的关键组件，贯穿数据仓库构建的整个过程，直接影响着数据仓库的构建、使用和维护。</p>
<ul>
<li><p>构建数据仓库的主要步骤之一是ETL。这时元数据将发挥重要的作用，它定义了源数据系统到数据仓库的映射、数据转换的规则、数据仓库的逻辑结构、数据更新的规则、数据导入历史记录以及装载周期等相关内容。数据抽取和转换的专家以及数据仓库管理员正是通过元数据高效地构建数据仓库。</p>
</li>
<li><p>用户在使用数据仓库时，通过元数据访问数据，明确数据项的含义以及定制报表。</p>
</li>
<li><p>数据仓库的规模及其复杂性离不开正确的元数据管理，包括增加或移除外部数据源，改变数据清洗方法，控制出错的查询以及安排备份等。</p>
<p><img src="file:///Users/lankangning/Downloads/%E8%AE%B2%E4%B9%89/md/assets/1561705895292.png?lastModify=1603853872" alt="1561705895292">    </p>
</li>
</ul>
<p>元数据可分为技术元数据和业务元数据。</p>
<p>技术元数据为开发和管理数据仓库的IT 人员使用，它描述了与数据仓库开发、管理和维护相关的数据，包括数据源信息、数据转换描述、数据仓库模型、数据清洗与更新规则、数据映射和访问权限等。</p>
<p>而业务元数据为管理层和业务分析人员服务，从业务角度描述数据，包括商务术语、数据仓库中有什么数据、数据的位置和数据的可用性等，帮助业务人员更好地理解数据仓库中哪些数据是可用的以及如何使用。</p>
<p>由上可见，元数据不仅定义了数据仓库中数据的模式、来源、抽取和转换规则等，而且是整个数据仓库系统运行的基础，元数据把数据仓库系统中各个松散的组件联系起来，组成了一个有机的整体。</p>
</li>
</ol>
<h3 id="2、hive"><a href="#2、hive" class="headerlink" title="2、hive"></a>2、hive</h3><ol>
<li><p>简介</p>
<p>hive是hadoop的一个数据仓库工具，</p>
<ol>
<li><p>可以将<strong>结构化的数据文件</strong>映射为<strong>一张数据库表</strong>，并提供sql查询功能。</p>
<p>hive元数据（存储在derby（默认）<strong>derby只能单用户</strong>，mysql（自定义））：</p>
<ol>
<li>记录表和文件之间的关系</li>
<li>记录表字段和文件字段的关系</li>
</ol>
</li>
<li><p>hive可以使用类sql语句对结构化数据文件进行查询</p>
<p>其本质是将SQL转化为MapReduce的任务进行运算，底层由HDFS来提供数据的存储。就是一个MapReduce的Client</p>
</li>
</ol>
</li>
<li><p>Hive 架构</p>
</li>
</ol>
<p><img src="/Users/lankangning/Downloads/%E8%AE%B2%E4%B9%89/md/assets/1561705918286.png" alt="1561705918286">    </p>
<ul>
<li><strong>用户接口：</strong> 包括CLI、JDBC/ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；JDBC/ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。</li>
<li><strong>元数据存储：</strong> 通常是存储在关系数据库如mysql/derby中。Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</li>
<li><strong>解释器（sql parser）、编译器（physical plan）、优化器（query optimizer）、执行器（execution）:</strong> 完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有MapReduce 调用执行。</li>
</ul>
<p>类sql语句——&gt;编译器——&gt;优化器——&gt;执行器——&gt;MapReduce——&gt;(读取）原数据文件——&gt;（返回）hdfs or 客户端</p>
<ol start="3">
<li>Hive 与 Hadoop 的关系</li>
</ol>
<p>Hive利用HDFS存储数据，利用MapReduce查询分析数据</p>
<p><img src="/Users/lankangning/Downloads/%E8%AE%B2%E4%B9%89/md/assets/1561705940248.png" alt="1561705940248">    </p>
<ol start="4">
<li>Hive与传统数据库对比</li>
</ol>
<p>hive用于海量数据的离线数据分析</p>
<p><img src="/Users/lankangning/Downloads/%E8%AE%B2%E4%B9%89/md/assets/1561705959607.png" alt="1561705959607">    </p>
<p>总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析</p>
<h3 id="3、安装hive"><a href="#3、安装hive" class="headerlink" title="3、安装hive"></a>3、安装hive</h3><ol>
<li><p>上传安装包并解压，在node03上</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/lib</span><br><span class="line">tar -xvf apache-hive-2.3.7-bin.tar.gz</span><br></pre></td></tr></table></figure></li>
<li><p>安装mysql</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在node03上验证安装</span></span><br><span class="line">mysql -u root -p</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在node01上验证远程登录mysql</span></span><br><span class="line">mysql -u root -p -h node03</span><br></pre></td></tr></table></figure></li>
<li><p>修改配置文件</p>
<p>hive-env.sh</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/lib/apache-hive-2.3.7-bin/conf</span><br><span class="line">cp hive-env.sh.template hive-env.sh</span><br><span class="line">vi hive-env.sh</span><br><span class="line">HADOOP_HOME=/usr/lib/hadoop-2.10.1</span><br><span class="line">export HIVE_CONF_DIR=/usr/lib/apache-hive-2.3.7-bin/conf</span><br></pre></td></tr></table></figure>
<p>Hive-site.xml</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/lib/apache-hive-2.3.7-bin/conf</span><br><span class="line">cp hive-default.sh.template hive-site.xml</span><br></pre></td></tr></table></figure>


</li>
</ol>
   <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="comment">&lt;!--连接数据库的用户名--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--指定mysql数据库密码--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>123<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--指定mysql中hive数据库访问路径，如果不存在，则创建一个--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node03:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--指定mysql连接的驱动--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--指定元数据是否校验--&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.schema.verification<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!--指定是否自动创建核心文件--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>datanucleus.schema.autoCreateAll<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--指定thrift绑定主机--&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">		<span class="tag">&lt;<span class="name">value</span>&gt;</span>node03<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol start="4">
<li><p>在安装目录lib下放一个mysql驱动包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 上maven下载，放进/usr/lib/apache-hive-2.3.7-bin/lib里</span></span><br><span class="line">mysql-connector-java-8.0.22.jar</span><br></pre></td></tr></table></figure></li>
<li><p>第五步：配置hive的环境变量</p>
<p>node03服务器执行以下命令配置hive的环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/profile</span><br><span class="line"></span><br><span class="line">export HIVE_HOME=/usr/lib/apache-hive-2.3.7-bin</span><br><span class="line">export PATH=:$HIVE_HOME/bin:$PATH</span><br></pre></td></tr></table></figure></li>
<li><p>配置hive元数据库hive到mysql</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mysql -u root -p</span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> create database hive;</span></span><br><span class="line"><span class="meta">mysql&gt;</span><span class="bash"> quit;</span></span><br><span class="line"></span><br><span class="line">cd /usr/lib/apache-hive-2.3.7-bin/bin</span><br><span class="line">schematool -initSchema -dbType mysql -verbose</span><br></pre></td></tr></table></figure>


</li>
</ol>
<ol start="7">
<li><p>hive交互方式</p>
<ul>
<li><p>bin/hive交互式操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/lib/apache-hive-2.3.7-bin</span><br><span class="line">bin/hive</span><br><span class="line"></span><br><span class="line"><span class="meta">hive&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>使用sql语句或者脚本交互</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hive -e &quot;create database if not exists mytest;&quot;</span><br><span class="line">bin/hive -f hive.sql</span><br></pre></td></tr></table></figure>
<p>2020年10月28日17:38:30</p>
</li>
</ul>
</li>
</ol>
<hr>
<h3 id="4、hive的基本操作"><a href="#4、hive的基本操作" class="headerlink" title="4、hive的基本操作"></a>4、hive的基本操作</h3><ol>
<li><p>数据库的操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">create database myhive;</span><br><span class="line">create database if not exists myhive;</span><br><span class="line"># 进入myhive数据库</span><br><span class="line">use myhive;</span><br><span class="line"># 创建数据库并指定位置</span><br><span class="line">create database myhive2 location &#39;&#x2F;myhive2&#39;;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 创建数据库并设置描述性的键值对信息（描述这个数据库）</span><br><span class="line">create database myhive3 with dbproperties(&#39;owner&#39;&#x3D;&#39;lan&#39;,&#39;date&#39;&#x3D;&#39;20201102&#39;);</span><br><span class="line"># 查看数据库描述信息</span><br><span class="line">describe database extended myhive3;</span><br><span class="line"># 修改数据库键值对信息</span><br><span class="line">alter database myhive3 set dbproperties(&#39;owner&#39;&#x3D;&#39;lanlan&#39;)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 删除数据库</span><br><span class="line">drop database myhive2;</span><br><span class="line"># 强制删除数据库，包含下面的数据表</span><br><span class="line">drop database myhive2 cascade;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>hive表存放位置，hive-site.xml当中的一个属性指定</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>数据表操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">create [external] table [if not exists] table_name(</span><br><span class="line"># 字段</span><br><span class="line">col_name data_type [comment &#39;字段描述信息&#39;]</span><br><span class="line">col_name data_type [comment &#39;字段描述信息&#39;]</span><br><span class="line">...）</span><br><span class="line">[comment &#39;表的描述信息&#39;]</span><br><span class="line"># hive表的分区，分文件夹存储</span><br><span class="line">[partitioned by (col_name data_type,...)]</span><br><span class="line"># hive表的分桶，分文件，类似于MapReduce的分区</span><br><span class="line">[clustered by (col_name,data_type,...)]</span><br><span class="line"># 排序，指定某一个字段排序，指定排序规则，asc升序，dsc降序</span><br><span class="line">[sorted by (col_name [asc|dsc],...) into num_buckets buckets]</span><br><span class="line"># 指定表数据文件的分隔符</span><br><span class="line">[row format row_format]</span><br><span class="line"># 指定表数据文件的存储类型 SEQUENCEFILE（压缩）,TEXTFILE(纯文本),RCFILE</span><br><span class="line">[stored as ...]</span><br><span class="line"># 指定表的存储路径，一般是hdfs</span><br><span class="line">[location &#39;...&#39;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># external 可以让用户创建一个外部表，在建表的同时指定一个指向实际数据的路径 （LOCATION），Hive 创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。</span><br></pre></td></tr></table></figure></li>
<li><p>内部表操作</p>
<p>建表入门:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">use myhive;</span><br><span class="line"></span><br><span class="line">create table stu(id int,name string);</span><br><span class="line"></span><br><span class="line">insert into stu values (1,&quot;zhangsan&quot;); #插入数据</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>select * from stu;</p>
</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">   创建表并指定字段之间的分隔符</span><br><span class="line">   </span><br><span class="line">   &#96;&#96;&#96;mysql</span><br><span class="line">create table if not exists stu2(id int ,name string) row format delimited fields terminated by &#39;\t&#39;;</span><br></pre></td></tr></table></figure>
<p>   创建表并指定表文件的存放路径</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists stu2(id int ,name string) row format delimited fields terminated by &#39;\t&#39; location &#39;&#x2F;user&#x2F;stu2&#39;;</span><br></pre></td></tr></table></figure>
<p>   根据查询结果创建表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">   # 通过复制表结构和表内容创建新表</span><br><span class="line">   create table stu3 as select * from stu2; </span><br><span class="line">select * from stu3</span><br></pre></td></tr></table></figure>
<p>   根据已经存在的表结构创建表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">create table stu4 like stu;</span><br><span class="line">desc stu4</span><br></pre></td></tr></table></figure>
<p>   查询表的详细信息</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc [formatted] stu3;</span><br></pre></td></tr></table></figure>
<p>   删除表</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">drop table stu2;</span><br></pre></td></tr></table></figure>
<ol start="4">
<li><p>外部表操作</p>
<p>外部表说明</p>
<p>外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉.</p>
<p>内部表和外部表的使用场</p>
<p>每天将收集到的网站日志定期流入HDFS文本文件。在外部表（原始日志表）的基础上做大量的统计分析，用到的中间表、结果表使用内部表存储，数据通过SELECT+INSERT进入内部表。</p>
<p>操作案</p>
<p>分别创建老师与学生表外部表，并向表中加载数据</p>
<p>创建老师表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> teacher (t_id string,t_name string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>创建学生表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> student (s_id string,s_name string,s_birth string , s_sex string ) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>加载数据（复制操作）</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/student.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure>
<p>加载数据并覆盖已有数据</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/student.csv&#x27;</span> overwrite  <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure>
<p>从hdfs文件系统向表中加载数据（需要提前将数据上传到hdfs文件系统）（剪切操作）</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/lib/hivedatas</span><br><span class="line">hdfs dfs -mkdir -p /hivedatas</span><br><span class="line">hdfs dfs -put techer.csv /hivedatas/</span><br><span class="line">load data inpath &#x27;/hivedatas/techer.csv&#x27; into table teacher;</span><br></pre></td></tr></table></figure>
<p>2020年11月02日14:03:16</p>
</li>
</ol>
<hr>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2020/12/29/Hadoop%E7%AC%94%E8%AE%B0/" data-id="ckj9nbv4800018kc90e1895sz" data-title="Hadoop笔记" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2020/12/29/hello-world/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Hello World</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/hadoop/" rel="tag">hadoop</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/hadoop/" style="font-size: 10px;">hadoop</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/12/">December 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/12/29/Hadoop%E7%AC%94%E8%AE%B0/">Hadoop笔记</a>
          </li>
        
          <li>
            <a href="/2020/12/29/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2020 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>